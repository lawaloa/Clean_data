<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.57">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Lawal’s note">
<meta name="dcterms.date" content="2024-10-03">

<title>COURSE 15: CLEANING DATA IN PYTHON</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="index_files/libs/clipboard/clipboard.min.js"></script>
<script src="index_files/libs/quarto-html/quarto.js"></script>
<script src="index_files/libs/quarto-html/popper.min.js"></script>
<script src="index_files/libs/quarto-html/tippy.umd.min.js"></script>
<script src="index_files/libs/quarto-html/anchor.min.js"></script>
<link href="index_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="index_files/libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="index_files/libs/bootstrap/bootstrap.min.js"></script>
<link href="index_files/libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="index_files/libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<style>

      .quarto-title-block .quarto-title-banner {
        background-image: url(image.jpg);
background-size: cover;
      }
</style>


</head>

<body>

<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">COURSE 15: CLEANING DATA IN PYTHON</h1>
                      </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>Lawal’s note </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">October 3, 2024</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="page-columns page-rows-contents page-layout-article">
<div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
  <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#chapter-1-data-type-constraints" id="toc-chapter-1-data-type-constraints" class="nav-link active" data-scroll-target="#chapter-1-data-type-constraints">CHAPTER 1: DATA TYPE CONSTRAINTS</a></li>
  <li><a href="#chapter-1.1-data-type-constraints" id="toc-chapter-1.1-data-type-constraints" class="nav-link" data-scroll-target="#chapter-1.1-data-type-constraints">Chapter 1.1: Data type constraints</a>
  <ul class="collapse">
  <li><a href="#course-outline" id="toc-course-outline" class="nav-link" data-scroll-target="#course-outline">Course outline</a></li>
  <li><a href="#why-do-we-need-to-clean-data" id="toc-why-do-we-need-to-clean-data" class="nav-link" data-scroll-target="#why-do-we-need-to-clean-data">Why do we need to clean data?</a></li>
  <li><a href="#data-type-constraints" id="toc-data-type-constraints" class="nav-link" data-scroll-target="#data-type-constraints">Data type constraints</a></li>
  <li><a href="#strings-to-integers" id="toc-strings-to-integers" class="nav-link" data-scroll-target="#strings-to-integers">Strings to integers</a></li>
  <li><a href="#the-assert-statement" id="toc-the-assert-statement" class="nav-link" data-scroll-target="#the-assert-statement">The assert statement</a></li>
  <li><a href="#numeric-or-categorical" id="toc-numeric-or-categorical" class="nav-link" data-scroll-target="#numeric-or-categorical">Numeric or categorical?</a></li>
  </ul></li>
  <li><a href="#exercise" id="toc-exercise" class="nav-link" data-scroll-target="#exercise">Exercise</a>
  <ul class="collapse">
  <li><a href="#numeric-data-or" id="toc-numeric-data-or" class="nav-link" data-scroll-target="#numeric-data-or">Numeric data or … ?</a></li>
  <li><a href="#instructions" id="toc-instructions" class="nav-link" data-scroll-target="#instructions">Instructions</a></li>
  <li><a href="#exercise-1.1" id="toc-exercise-1.1" class="nav-link" data-scroll-target="#exercise-1.1">Exercise 1.1</a></li>
  <li><a href="#summing-strings-and-concatenating-numbers" id="toc-summing-strings-and-concatenating-numbers" class="nav-link" data-scroll-target="#summing-strings-and-concatenating-numbers">Summing strings and concatenating numbers</a></li>
  <li><a href="#instructions-1" id="toc-instructions-1" class="nav-link" data-scroll-target="#instructions-1">Instructions</a></li>
  </ul></li>
  <li><a href="#chapter-1.2-data-range-constraints" id="toc-chapter-1.2-data-range-constraints" class="nav-link" data-scroll-target="#chapter-1.2-data-range-constraints">Chapter 1.2: Data range constraints</a>
  <ul class="collapse">
  <li><a href="#motivation" id="toc-motivation" class="nav-link" data-scroll-target="#motivation">Motivation</a></li>
  <li><a href="#how-to-deal-with-out-of-range-data" id="toc-how-to-deal-with-out-of-range-data" class="nav-link" data-scroll-target="#how-to-deal-with-out-of-range-data">How to deal with out of range data?</a></li>
  <li><a href="#movie-example" id="toc-movie-example" class="nav-link" data-scroll-target="#movie-example">Movie example</a></li>
  <li><a href="#date-range-example" id="toc-date-range-example" class="nav-link" data-scroll-target="#date-range-example">Date range example</a></li>
  </ul></li>
  <li><a href="#exercise-1.2.1" id="toc-exercise-1.2.1" class="nav-link" data-scroll-target="#exercise-1.2.1">Exercise 1.2.1</a>
  <ul class="collapse">
  <li><a href="#tire-size-constraints" id="toc-tire-size-constraints" class="nav-link" data-scroll-target="#tire-size-constraints">Tire size constraints</a></li>
  <li><a href="#instructions-2" id="toc-instructions-2" class="nav-link" data-scroll-target="#instructions-2">Instructions</a></li>
  </ul></li>
  <li><a href="#exercise-1.2.2" id="toc-exercise-1.2.2" class="nav-link" data-scroll-target="#exercise-1.2.2">Exercise 1.2.2</a>
  <ul class="collapse">
  <li><a href="#back-to-the-future" id="toc-back-to-the-future" class="nav-link" data-scroll-target="#back-to-the-future">Back to the future</a></li>
  <li><a href="#instructions-3" id="toc-instructions-3" class="nav-link" data-scroll-target="#instructions-3">Instructions</a></li>
  </ul></li>
  <li><a href="#chapter-1.3-uniqueness-constraints" id="toc-chapter-1.3-uniqueness-constraints" class="nav-link" data-scroll-target="#chapter-1.3-uniqueness-constraints">Chapter 1.3: Uniqueness constraints</a>
  <ul class="collapse">
  <li><a href="#what-are-duplicate-values" id="toc-what-are-duplicate-values" class="nav-link" data-scroll-target="#what-are-duplicate-values">What are duplicate values?</a></li>
  <li><a href="#why-do-they-happen" id="toc-why-do-they-happen" class="nav-link" data-scroll-target="#why-do-they-happen">Why do they happen?</a></li>
  <li><a href="#how-to-find-duplicate-values" id="toc-how-to-find-duplicate-values" class="nav-link" data-scroll-target="#how-to-find-duplicate-values">How to find duplicate values?</a></li>
  <li><a href="#how-to-treat-duplicate-values" id="toc-how-to-treat-duplicate-values" class="nav-link" data-scroll-target="#how-to-treat-duplicate-values">How to treat duplicate values?</a></li>
  </ul></li>
  <li><a href="#exercise-1.3.1" id="toc-exercise-1.3.1" class="nav-link" data-scroll-target="#exercise-1.3.1">Exercise 1.3.1</a>
  <ul class="collapse">
  <li><a href="#finding-duplicates" id="toc-finding-duplicates" class="nav-link" data-scroll-target="#finding-duplicates">Finding duplicates</a></li>
  <li><a href="#instructions-4" id="toc-instructions-4" class="nav-link" data-scroll-target="#instructions-4">Instructions</a></li>
  <li><a href="#exercise-1.3.2" id="toc-exercise-1.3.2" class="nav-link" data-scroll-target="#exercise-1.3.2">Exercise 1.3.2</a></li>
  </ul></li>
  <li><a href="#chapter-2-text-and-categorical-data-problems" id="toc-chapter-2-text-and-categorical-data-problems" class="nav-link" data-scroll-target="#chapter-2-text-and-categorical-data-problems">CHAPTER 2: TEXT AND CATEGORICAL DATA PROBLEMS</a></li>
  <li><a href="#chapter-2.1-membership-constraints" id="toc-chapter-2.1-membership-constraints" class="nav-link" data-scroll-target="#chapter-2.1-membership-constraints">Chapter 2.1: Membership constraints</a>
  <ul class="collapse">
  <li><a href="#in-this-chapter" id="toc-in-this-chapter" class="nav-link" data-scroll-target="#in-this-chapter">In this chapter</a></li>
  <li><a href="#categories-and-membership-constraints" id="toc-categories-and-membership-constraints" class="nav-link" data-scroll-target="#categories-and-membership-constraints">Categories and membership constraints</a></li>
  <li><a href="#why-could-we-have-these-problems" id="toc-why-could-we-have-these-problems" class="nav-link" data-scroll-target="#why-could-we-have-these-problems">Why could we have these problems?</a></li>
  <li><a href="#how-do-we-treat-these-problems" id="toc-how-do-we-treat-these-problems" class="nav-link" data-scroll-target="#how-do-we-treat-these-problems">How do we treat these problems?</a></li>
  <li><a href="#a-note-on-joins" id="toc-a-note-on-joins" class="nav-link" data-scroll-target="#a-note-on-joins">A note on joins</a></li>
  <li><a href="#a-left-anti-join-on-blood-types" id="toc-a-left-anti-join-on-blood-types" class="nav-link" data-scroll-target="#a-left-anti-join-on-blood-types">A left anti join on blood types</a></li>
  <li><a href="#an-inner-join-on-blood-types" id="toc-an-inner-join-on-blood-types" class="nav-link" data-scroll-target="#an-inner-join-on-blood-types">An inner join on blood types</a></li>
  <li><a href="#finding-inconsistent-categories" id="toc-finding-inconsistent-categories" class="nav-link" data-scroll-target="#finding-inconsistent-categories">Finding inconsistent categories**</a></li>
  <li><a href="#dropping-inconsistent-categories" id="toc-dropping-inconsistent-categories" class="nav-link" data-scroll-target="#dropping-inconsistent-categories">Dropping inconsistent categories</a></li>
  </ul></li>
  <li><a href="#exercise-2.1" id="toc-exercise-2.1" class="nav-link" data-scroll-target="#exercise-2.1">Exercise 2.1</a>
  <ul class="collapse">
  <li><a href="#finding-consistency" id="toc-finding-consistency" class="nav-link" data-scroll-target="#finding-consistency">Finding consistency</a></li>
  <li><a href="#instructions-5" id="toc-instructions-5" class="nav-link" data-scroll-target="#instructions-5">Instructions</a></li>
  </ul></li>
  <li><a href="#chapter-2.2-categorical-variables" id="toc-chapter-2.2-categorical-variables" class="nav-link" data-scroll-target="#chapter-2.2-categorical-variables">Chapter 2.2: Categorical variables</a>
  <ul class="collapse">
  <li><a href="#what-type-of-errors-could-we-have" id="toc-what-type-of-errors-could-we-have" class="nav-link" data-scroll-target="#what-type-of-errors-could-we-have">What type of errors could we have?</a></li>
  <li><a href="#value-consistency" id="toc-value-consistency" class="nav-link" data-scroll-target="#value-consistency">Value consistency</a></li>
  <li><a href="#collapsing-data-into-categories" id="toc-collapsing-data-into-categories" class="nav-link" data-scroll-target="#collapsing-data-into-categories">Collapsing data into categories</a></li>
  </ul></li>
  <li><a href="#exercise-2.2.1" id="toc-exercise-2.2.1" class="nav-link" data-scroll-target="#exercise-2.2.1">Exercise 2.2.1</a>
  <ul class="collapse">
  <li><a href="#inconsistent-categories" id="toc-inconsistent-categories" class="nav-link" data-scroll-target="#inconsistent-categories">Inconsistent categories</a></li>
  <li><a href="#instructions-6" id="toc-instructions-6" class="nav-link" data-scroll-target="#instructions-6">Instructions</a></li>
  </ul></li>
  <li><a href="#exercise-2.2.2" id="toc-exercise-2.2.2" class="nav-link" data-scroll-target="#exercise-2.2.2">Exercise 2.2.2</a>
  <ul class="collapse">
  <li><a href="#remapping-categories" id="toc-remapping-categories" class="nav-link" data-scroll-target="#remapping-categories">Remapping categories</a></li>
  <li><a href="#instructions-7" id="toc-instructions-7" class="nav-link" data-scroll-target="#instructions-7">Instructions</a></li>
  </ul></li>
  <li><a href="#chapter-2.3-cleaning-text-data" id="toc-chapter-2.3-cleaning-text-data" class="nav-link" data-scroll-target="#chapter-2.3-cleaning-text-data">Chapter 2.3: Cleaning text data</a>
  <ul class="collapse">
  <li><a href="#what-is-text-data" id="toc-what-is-text-data" class="nav-link" data-scroll-target="#what-is-text-data">What is text data?</a></li>
  <li><a href="#example" id="toc-example" class="nav-link" data-scroll-target="#example">Example</a></li>
  <li><a href="#fixing-the-phone-number-column" id="toc-fixing-the-phone-number-column" class="nav-link" data-scroll-target="#fixing-the-phone-number-column">Fixing the phone number column</a></li>
  <li><a href="#but-what-about-more-complicated-examples" id="toc-but-what-about-more-complicated-examples" class="nav-link" data-scroll-target="#but-what-about-more-complicated-examples">But what about more complicated examples?</a></li>
  <li><a href="#regular-expressions-in-action" id="toc-regular-expressions-in-action" class="nav-link" data-scroll-target="#regular-expressions-in-action">Regular expressions in action</a></li>
  </ul></li>
  <li><a href="#exercise-2.3.1" id="toc-exercise-2.3.1" class="nav-link" data-scroll-target="#exercise-2.3.1">Exercise 2.3.1</a>
  <ul class="collapse">
  <li><a href="#removing-titles-and-taking-names" id="toc-removing-titles-and-taking-names" class="nav-link" data-scroll-target="#removing-titles-and-taking-names">Removing titles and taking names</a></li>
  <li><a href="#instructions-8" id="toc-instructions-8" class="nav-link" data-scroll-target="#instructions-8">Instructions</a></li>
  </ul></li>
  <li><a href="#exercise-2.3.2" id="toc-exercise-2.3.2" class="nav-link" data-scroll-target="#exercise-2.3.2">Exercise 2.3.2</a>
  <ul class="collapse">
  <li><a href="#keeping-it-descriptive" id="toc-keeping-it-descriptive" class="nav-link" data-scroll-target="#keeping-it-descriptive">Keeping it descriptive</a></li>
  <li><a href="#instructions-9" id="toc-instructions-9" class="nav-link" data-scroll-target="#instructions-9">Instructions</a></li>
  </ul></li>
  <li><a href="#chapter-3-advanced-data-problems" id="toc-chapter-3-advanced-data-problems" class="nav-link" data-scroll-target="#chapter-3-advanced-data-problems">CHAPTER 3: ADVANCED DATA PROBLEMS</a></li>
  <li><a href="#chapter-3.1-uniformity" id="toc-chapter-3.1-uniformity" class="nav-link" data-scroll-target="#chapter-3.1-uniformity">Chapter 3.1: Uniformity</a>
  <ul class="collapse">
  <li><a href="#in-this-chapter-1" id="toc-in-this-chapter-1" class="nav-link" data-scroll-target="#in-this-chapter-1">In this chapter</a></li>
  <li><a href="#data-range-constraints" id="toc-data-range-constraints" class="nav-link" data-scroll-target="#data-range-constraints">Data range constraints</a></li>
  <li><a href="#uniformity" id="toc-uniformity" class="nav-link" data-scroll-target="#uniformity">Uniformity</a></li>
  <li><a href="#an-example" id="toc-an-example" class="nav-link" data-scroll-target="#an-example">An example</a></li>
  <li><a href="#insert-title-here" id="toc-insert-title-here" class="nav-link" data-scroll-target="#insert-title-here">Insert title here…</a></li>
  <li><a href="#treating-temperature-data" id="toc-treating-temperature-data" class="nav-link" data-scroll-target="#treating-temperature-data">Treating temperature data</a></li>
  <li><a href="#treating-date-data" id="toc-treating-date-data" class="nav-link" data-scroll-target="#treating-date-data">Treating date data</a></li>
  <li><a href="#datetime-formatting" id="toc-datetime-formatting" class="nav-link" data-scroll-target="#datetime-formatting">Datetime formatting</a></li>
  <li><a href="#treating-date-data-1" id="toc-treating-date-data-1" class="nav-link" data-scroll-target="#treating-date-data-1">Treating date data</a></li>
  <li><a href="#treating-ambiguous-date-data" id="toc-treating-ambiguous-date-data" class="nav-link" data-scroll-target="#treating-ambiguous-date-data">Treating ambiguous date data</a></li>
  </ul></li>
  <li><a href="#exercise-3.1.1" id="toc-exercise-3.1.1" class="nav-link" data-scroll-target="#exercise-3.1.1">Exercise 3.1.1</a>
  <ul class="collapse">
  <li><a href="#uniform-currencies" id="toc-uniform-currencies" class="nav-link" data-scroll-target="#uniform-currencies">Uniform currencies</a></li>
  <li><a href="#instructions-10" id="toc-instructions-10" class="nav-link" data-scroll-target="#instructions-10">Instructions</a></li>
  </ul></li>
  <li><a href="#exercise-3.1.2" id="toc-exercise-3.1.2" class="nav-link" data-scroll-target="#exercise-3.1.2">Exercise 3.1.2</a>
  <ul class="collapse">
  <li><a href="#uniform-dates" id="toc-uniform-dates" class="nav-link" data-scroll-target="#uniform-dates">Uniform dates</a></li>
  <li><a href="#instructions-11" id="toc-instructions-11" class="nav-link" data-scroll-target="#instructions-11">Instructions</a></li>
  </ul></li>
  <li><a href="#chapter-3.2-cross-field-validation" id="toc-chapter-3.2-cross-field-validation" class="nav-link" data-scroll-target="#chapter-3.2-cross-field-validation">Chapter 3.2: Cross field validation</a>
  <ul class="collapse">
  <li><a href="#motivation-1" id="toc-motivation-1" class="nav-link" data-scroll-target="#motivation-1">Motivation</a></li>
  <li><a href="#cross-field-validation" id="toc-cross-field-validation" class="nav-link" data-scroll-target="#cross-field-validation">Cross field validation</a></li>
  <li><a href="#what-to-do-when-we-catch-inconsistencies" id="toc-what-to-do-when-we-catch-inconsistencies" class="nav-link" data-scroll-target="#what-to-do-when-we-catch-inconsistencies">What to do when we catch inconsistencies?</a></li>
  </ul></li>
  <li><a href="#exercise-3.2" id="toc-exercise-3.2" class="nav-link" data-scroll-target="#exercise-3.2">Exercise 3.2</a>
  <ul class="collapse">
  <li><a href="#hows-our-data-integrity" id="toc-hows-our-data-integrity" class="nav-link" data-scroll-target="#hows-our-data-integrity">How’s our data integrity?</a></li>
  <li><a href="#instructions-12" id="toc-instructions-12" class="nav-link" data-scroll-target="#instructions-12">Instructions</a></li>
  </ul></li>
  <li><a href="#chapter-3.3-completeness" id="toc-chapter-3.3-completeness" class="nav-link" data-scroll-target="#chapter-3.3-completeness">Chapter 3.3: Completeness</a>
  <ul class="collapse">
  <li><a href="#what-is-missing-data" id="toc-what-is-missing-data" class="nav-link" data-scroll-target="#what-is-missing-data">What is missing data?</a></li>
  <li><a href="#airquality-example" id="toc-airquality-example" class="nav-link" data-scroll-target="#airquality-example">Airquality example</a></li>
  <li><a href="#missingno" id="toc-missingno" class="nav-link" data-scroll-target="#missingno">Missingno</a></li>
  <li><a href="#airquality-example-1" id="toc-airquality-example-1" class="nav-link" data-scroll-target="#airquality-example-1">Airquality example</a></li>
  <li><a href="#insert-title-here-1" id="toc-insert-title-here-1" class="nav-link" data-scroll-target="#insert-title-here-1">Insert title here…</a></li>
  <li><a href="#missingness-types" id="toc-missingness-types" class="nav-link" data-scroll-target="#missingness-types">Missingness types</a></li>
  <li><a href="#how-to-deal-with-missing-data" id="toc-how-to-deal-with-missing-data" class="nav-link" data-scroll-target="#how-to-deal-with-missing-data">How to deal with missing data?</a></li>
  <li><a href="#dealing-with-missing-data" id="toc-dealing-with-missing-data" class="nav-link" data-scroll-target="#dealing-with-missing-data">Dealing with missing data</a></li>
  <li><a href="#dropping-missing-values" id="toc-dropping-missing-values" class="nav-link" data-scroll-target="#dropping-missing-values">Dropping missing values</a></li>
  <li><a href="#replacing-with-statistical-measures" id="toc-replacing-with-statistical-measures" class="nav-link" data-scroll-target="#replacing-with-statistical-measures">Replacing with statistical measures</a></li>
  </ul></li>
  <li><a href="#exercise-3.3.1" id="toc-exercise-3.3.1" class="nav-link" data-scroll-target="#exercise-3.3.1">Exercise 3.3.1</a>
  <ul class="collapse">
  <li><a href="#missing-investors" id="toc-missing-investors" class="nav-link" data-scroll-target="#missing-investors">Missing investors</a></li>
  <li><a href="#instructions-13" id="toc-instructions-13" class="nav-link" data-scroll-target="#instructions-13">Instructions</a></li>
  <li><a href="#exercise-3.3.2" id="toc-exercise-3.3.2" class="nav-link" data-scroll-target="#exercise-3.3.2">Exercise 3.3.2</a></li>
  <li><a href="#follow-the-money" id="toc-follow-the-money" class="nav-link" data-scroll-target="#follow-the-money">Follow the money</a></li>
  <li><a href="#instructions-14" id="toc-instructions-14" class="nav-link" data-scroll-target="#instructions-14">Instructions</a></li>
  </ul></li>
  <li><a href="#there-are-only-8-and-92-rows-affected-by-inconsistent-inv_amount-and-age-values-respectively.-in-this-case-its-best-to-investigate-the-underlying-data-sources-before-deciding-on-a-course-of-action" id="toc-there-are-only-8-and-92-rows-affected-by-inconsistent-inv_amount-and-age-values-respectively.-in-this-case-its-best-to-investigate-the-underlying-data-sources-before-deciding-on-a-course-of-action" class="nav-link" data-scroll-target="#there-are-only-8-and-92-rows-affected-by-inconsistent-inv_amount-and-age-values-respectively.-in-this-case-its-best-to-investigate-the-underlying-data-sources-before-deciding-on-a-course-of-action">There are only 8 and 92 rows affected by <em>inconsistent inv_amount and age values</em>, respectively. In this case, it’s best to investigate the underlying data sources before deciding on a course of action!</a>
  <ul class="collapse">
  <li><a href="#chapter-4-record-linkage" id="toc-chapter-4-record-linkage" class="nav-link" data-scroll-target="#chapter-4-record-linkage">CHAPTER 4: RECORD LINKAGE</a></li>
  <li><a href="#chapter-4.1-comparing-strings" id="toc-chapter-4.1-comparing-strings" class="nav-link" data-scroll-target="#chapter-4.1-comparing-strings">Chapter 4.1: Comparing strings</a>
  <ul class="collapse">
  <li><a href="#minimum-edit-distance-algorithms" id="toc-minimum-edit-distance-algorithms" class="nav-link" data-scroll-target="#minimum-edit-distance-algorithms">Minimum edit distance algorithms</a></li>
  <li><a href="#simple-string-comparison" id="toc-simple-string-comparison" class="nav-link" data-scroll-target="#simple-string-comparison">Simple string comparison</a></li>
  <li><a href="#partial-strings-and-different-orderings" id="toc-partial-strings-and-different-orderings" class="nav-link" data-scroll-target="#partial-strings-and-different-orderings">Partial strings and different orderings</a></li>
  <li><a href="#comparison-with-arrays" id="toc-comparison-with-arrays" class="nav-link" data-scroll-target="#comparison-with-arrays">Comparison with arrays</a></li>
  <li><a href="#collapsing-categories-with-string-similarity" id="toc-collapsing-categories-with-string-similarity" class="nav-link" data-scroll-target="#collapsing-categories-with-string-similarity">Collapsing categories with string similarity</a></li>
  <li><a href="#collapsing-categories-with-string-matching" id="toc-collapsing-categories-with-string-matching" class="nav-link" data-scroll-target="#collapsing-categories-with-string-matching">Collapsing categories with string matching</a></li>
  <li><a href="#collapsing-all-of-the-state" id="toc-collapsing-all-of-the-state" class="nav-link" data-scroll-target="#collapsing-all-of-the-state">Collapsing all of the state</a></li>
  <li><a href="#record-linkage" id="toc-record-linkage" class="nav-link" data-scroll-target="#record-linkage">Record linkage</a></li>
  </ul></li>
  <li><a href="#exercise-4.1.1" id="toc-exercise-4.1.1" class="nav-link" data-scroll-target="#exercise-4.1.1">Exercise 4.1.1</a>
  <ul class="collapse">
  <li><a href="#the-cutoff-point" id="toc-the-cutoff-point" class="nav-link" data-scroll-target="#the-cutoff-point">The cutoff point</a></li>
  <li><a href="#instructions-15" id="toc-instructions-15" class="nav-link" data-scroll-target="#instructions-15">Instructions</a></li>
  <li><a href="#remapping-categories-ii" id="toc-remapping-categories-ii" class="nav-link" data-scroll-target="#remapping-categories-ii">Remapping categories II</a></li>
  <li><a href="#instructions-16" id="toc-instructions-16" class="nav-link" data-scroll-target="#instructions-16">Instructions</a></li>
  </ul></li>
  <li><a href="#chapter-4.2-generating-pairs" id="toc-chapter-4.2-generating-pairs" class="nav-link" data-scroll-target="#chapter-4.2-generating-pairs">Chapter 4.2: Generating pairs</a>
  <ul class="collapse">
  <li><a href="#motivation-2" id="toc-motivation-2" class="nav-link" data-scroll-target="#motivation-2">Motivation</a></li>
  <li><a href="#when-joins-wont-work" id="toc-when-joins-wont-work" class="nav-link" data-scroll-target="#when-joins-wont-work">When joins won’t work</a></li>
  <li><a href="#record-linkage-1" id="toc-record-linkage-1" class="nav-link" data-scroll-target="#record-linkage-1">Record linkage</a></li>
  <li><a href="#our-dataframes" id="toc-our-dataframes" class="nav-link" data-scroll-target="#our-dataframes">Our DataFrames</a></li>
  <li><a href="#generating-pairs" id="toc-generating-pairs" class="nav-link" data-scroll-target="#generating-pairs">Generating pairs</a></li>
  <li><a href="#blocking" id="toc-blocking" class="nav-link" data-scroll-target="#blocking">Blocking</a></li>
  <li><a href="#generating-pairs-1" id="toc-generating-pairs-1" class="nav-link" data-scroll-target="#generating-pairs-1">Generating pairs</a></li>
  <li><a href="#comparing-the-dataframes" id="toc-comparing-the-dataframes" class="nav-link" data-scroll-target="#comparing-the-dataframes">Comparing the DataFrames</a></li>
  <li><a href="#finding-matching-pairs" id="toc-finding-matching-pairs" class="nav-link" data-scroll-target="#finding-matching-pairs">Finding matching pairs</a></li>
  <li><a href="#finding-the-only-pairs-we-want" id="toc-finding-the-only-pairs-we-want" class="nav-link" data-scroll-target="#finding-the-only-pairs-we-want">Finding the only pairs we want</a></li>
  </ul></li>
  <li><a href="#exercise-4.2.1" id="toc-exercise-4.2.1" class="nav-link" data-scroll-target="#exercise-4.2.1">Exercise 4.2.1</a>
  <ul class="collapse">
  <li><a href="#pairs-of-restaurants" id="toc-pairs-of-restaurants" class="nav-link" data-scroll-target="#pairs-of-restaurants">Pairs of restaurants</a></li>
  <li><a href="#instructions-17" id="toc-instructions-17" class="nav-link" data-scroll-target="#instructions-17">Instructions</a></li>
  </ul></li>
  <li><a href="#exercise-4.2.2" id="toc-exercise-4.2.2" class="nav-link" data-scroll-target="#exercise-4.2.2">Exercise 4.2.2</a>
  <ul class="collapse">
  <li><a href="#similar-restaurants" id="toc-similar-restaurants" class="nav-link" data-scroll-target="#similar-restaurants">Similar restaurants</a></li>
  <li><a href="#instructions-18" id="toc-instructions-18" class="nav-link" data-scroll-target="#instructions-18">Instructions</a></li>
  </ul></li>
  <li><a href="#chapter-4.3-linking-dataframes" id="toc-chapter-4.3-linking-dataframes" class="nav-link" data-scroll-target="#chapter-4.3-linking-dataframes">Chapter 4.3: Linking DataFrames</a>
  <ul class="collapse">
  <li><a href="#record-linkage-2" id="toc-record-linkage-2" class="nav-link" data-scroll-target="#record-linkage-2">Record linkage</a></li>
  <li><a href="#our-dataframes-1" id="toc-our-dataframes-1" class="nav-link" data-scroll-target="#our-dataframes-1">Our DataFrames</a></li>
  <li><a href="#what-weve-already-done" id="toc-what-weve-already-done" class="nav-link" data-scroll-target="#what-weve-already-done">What we’ve already done</a></li>
  <li><a href="#what-were-doing-now" id="toc-what-were-doing-now" class="nav-link" data-scroll-target="#what-were-doing-now">What we’re doing now</a></li>
  <li><a href="#our-potential-matches" id="toc-our-potential-matches" class="nav-link" data-scroll-target="#our-potential-matches">Our potential matches</a></li>
  <li><a href="#probable-matches" id="toc-probable-matches" class="nav-link" data-scroll-target="#probable-matches">Probable matches</a></li>
  <li><a href="#get-the-indices" id="toc-get-the-indices" class="nav-link" data-scroll-target="#get-the-indices">Get the indices</a></li>
  </ul></li>
  <li><a href="#exercise-4.3" id="toc-exercise-4.3" class="nav-link" data-scroll-target="#exercise-4.3">Exercise 4.3</a>
  <ul class="collapse">
  <li><a href="#linking-them-together" id="toc-linking-them-together" class="nav-link" data-scroll-target="#linking-them-together">Linking them together!</a></li>
  <li><a href="#instructions-19" id="toc-instructions-19" class="nav-link" data-scroll-target="#instructions-19">Instructions</a></li>
  </ul></li>
  <li><a href="#explore-datasets" id="toc-explore-datasets" class="nav-link" data-scroll-target="#explore-datasets">EXPLORE DATASETS</a></li>
  </ul></li>
  </ul>
<div class="quarto-alternate-formats"><h2>Other Formats</h2><ul><li><a href="index.pdf"><i class="bi bi-file-pdf"></i>PDF</a></li><li><a href="index.docx"><i class="bi bi-file-word"></i>MS Word</a></li></ul></div></nav>
</div>
<main class="content quarto-banner-title-block" id="quarto-document-content">





<section id="chapter-1-data-type-constraints" class="level2">
<h2 class="anchored" data-anchor-id="chapter-1-data-type-constraints">CHAPTER 1: DATA TYPE CONSTRAINTS</h2>
</section>
<section id="chapter-1.1-data-type-constraints" class="level2">
<h2 class="anchored" data-anchor-id="chapter-1.1-data-type-constraints">Chapter 1.1: Data type constraints</h2>
<p>Hi and welcome! My name is Adel, and I’ll be your host as we learn how to clean data in Python.</p>
<section id="course-outline" class="level3">
<h3 class="anchored" data-anchor-id="course-outline">Course outline</h3>
<p>In this course, we’re going to understand how to diagnose different problems in our data and how they can can come up during our workflow. We will also understand the side effects of not treating our data correctly and various ways to address different types of dirty data. In this chapter, we’re going to discuss the most common data problems you may encounter and how to address them. So let’s get started!</p>
</section>
<section id="why-do-we-need-to-clean-data" class="level3">
<h3 class="anchored" data-anchor-id="why-do-we-need-to-clean-data">Why do we need to clean data?</h3>
<p>To understand why we need to clean data, let’s remind ourselves of the data science workflow. In a typical data science workflow, we usually access our raw data, explore and process it, develop insights using visualizations or predictive models, and finally report these insights with dashboards or reports. Dirty data can appear because of duplicate values, mis-spellings, data type parsing errors and legacy systems. Without making sure that data is properly cleaned in the exploration and processing phase, we will surely compromise the insights and reports subsequently generated. As the old adage says, garbage in garbage out.</p>
</section>
<section id="data-type-constraints" class="level3">
<h3 class="anchored" data-anchor-id="data-type-constraints">Data type constraints</h3>
<p>When working with data, there are various types that we may encounter along the way. We could be working with text data, integers, decimals, dates, zip codes, and others. Luckily, Python has specific data type objects for various data types that you’re probably familiar with by now. This makes it much easier to manipulate these various data types in Python. As such, before preparing to analyze and extract insights from our data, we need to make sure our variables have the correct data types, other wise we risk compromising our analysis.</p>
</section>
<section id="strings-to-integers" class="level3">
<h3 class="anchored" data-anchor-id="strings-to-integers">Strings to integers</h3>
<p>Let’s take a look at the following example. Here’s the head of a DataFrame containing revenue generated and quantity of items sold for a sales order. We want to calculate the total revenue generated by all sales orders. As you can see, the Revenue column has the dollar sign on the right hand side. A close inspection of the DataFrame column’s data types using the <code>.dtypes</code> attribute returns object for the Revenue column, which is what pandas uses to store strings. We can also check the data types as well as the number of missing values per column in a DataFrame, by using the <code>.info()</code> method. Since the Revenue column is a string, summing across all sales orders returns one large concatenated string containing each row’s string. To fix this, we need to first remove the $ sign from the string so that pandas is able to convert the strings into numbers without error. We do this with the .str.strip() method, while specifying the string we want to strip as an argument, which is in this case the dollar sign. Since our dollar values do not contain decimals, we then convert the Revenue column to an integer by using the <code>.astype()</code> method, specifying the desired data type as argument. Had our revenue values been decimal, we would have converted the Revenue column to float. We can make sure that the Revenue column is now an integer by using the assert statement, which takes in a condition as input, as returns nothing if that condition is met, and an error if it is not.</p>
</section>
<section id="the-assert-statement" class="level3">
<h3 class="anchored" data-anchor-id="the-assert-statement">The assert statement</h3>
<p>For example, here we are testing the equality that 1+1 equals 2. Since it is the case, the assert statement returns nothing. However, when testing the equality 1+1 equals 3, we receive an assertionerror. You can test almost anything you can imagine of by using assert, and we’ll see more ways to utilize it as we go along the course.</p>
</section>
<section id="numeric-or-categorical" class="level3">
<h3 class="anchored" data-anchor-id="numeric-or-categorical">Numeric or categorical?</h3>
<p>A common type of data seems numeric but actually represents categories with a finite set of possible categories. This is called categorical data. We will look more closely at categorical data in Chapter 2, but let’s take a look at this example. Here we have a marriage status column, which is represented by 0 for never married, 1 for married, 2 for separated, and 3 for divorced. However it will be imported of type integer, which could lead to misleading results when trying to extract some statistical summaries. We can solve this by using the same .astype() method seen earlier, but this time specifying the category data type. When applying the describe again, we see that the summary statistics are much more aligned with that of a categorical variable, discussing the number of observations, number of unique values, most frequent category instead of mean and standard deviation.</p>
</section>
</section>
<section id="exercise" class="level2">
<h2 class="anchored" data-anchor-id="exercise">Exercise</h2>
<section id="numeric-data-or" class="level3">
<h3 class="anchored" data-anchor-id="numeric-data-or">Numeric data or … ?</h3>
<p>In this exercise, and throughout this chapter, you’ll be working with bicycle <code>ride sharing</code> data in San Francisco called <code>ride_sharing</code>. It contains information on the start and end stations, the trip duration, and some user information for a bike sharing service.</p>
<p>The <code>user_type</code> column contains information on whether a user is taking a free ride and takes on the following values:</p>
<ul>
<li><code>1</code> for free riders.</li>
<li><code>2</code> for pay per ride.</li>
<li><code>3</code> for monthly subscribers.</li>
</ul>
<p>In this instance, you will print the information of <code>ride_sharing</code> using <code>.info()</code> and see a firsthand example of how an incorrect data type can flaw your analysis of the dataset.</p>
</section>
<section id="instructions" class="level3">
<h3 class="anchored" data-anchor-id="instructions">Instructions</h3>
<ul>
<li>Print the information of <code>ride_sharing</code>.</li>
<li>Use <code>.describe()</code> to print the summary statistics of the <code>user_type</code> column from <code>ride_sharing</code>.</li>
<li>Convert <code>user_type</code> into categorical by assigning it the <code>'category'</code> data type and store it in the <code>user_type_cat column</code>.</li>
<li>Make sure you converted <code>user_type_cat</code> correctly by using an <code>assert</code> statement.</li>
</ul>
</section>
<section id="exercise-1.1" class="level3">
<h3 class="anchored" data-anchor-id="exercise-1.1">Exercise 1.1</h3>
</section>
<section id="summing-strings-and-concatenating-numbers" class="level3">
<h3 class="anchored" data-anchor-id="summing-strings-and-concatenating-numbers">Summing strings and concatenating numbers</h3>
<p>In the previous exercise, you were able to identify that <code>category</code> is the correct data type for <code>user_type</code> and convert it in order to extract relevant statistical summaries that shed light on the distribution of <code>user_type</code>.</p>
<p>Another common data type problem is importing what should be numerical values as strings, as mathematical operations such as summing and multiplication lead to string concatenation, not numerical outputs.</p>
<p>In this exercise, you’ll be converting the string column <code>duration</code> to the type <code>int</code>. Before that however, you will need to make sure to strip <code>"minutes"</code> from the column in order to make sure <code>pandas</code> reads it as numerical.</p>
</section>
<section id="instructions-1" class="level3">
<h3 class="anchored" data-anchor-id="instructions-1">Instructions</h3>
<ul>
<li>Use the <code>.strip()</code> method to strip <code>duration</code> of <code>"minutes"</code> and store it in the <code>duration_trim</code> column.</li>
<li>Convert <code>duration_trim</code> to <code>int</code> and store it in the <code>duration_time</code> column.</li>
<li>Write an <code>assert</code> statement that checks if <code>duration_time</code>’s data type is now an <code>int</code>. Print the average ride duration.</li>
</ul>
</section>
</section>
<section id="chapter-1.2-data-range-constraints" class="level2">
<h2 class="anchored" data-anchor-id="chapter-1.2-data-range-constraints">Chapter 1.2: Data range constraints</h2>
<p>Hi and welcome back! In this lesson, we’re going to discuss data that should fall within a range.</p>
<section id="motivation" class="level3">
<h3 class="anchored" data-anchor-id="motivation">Motivation</h3>
<p>Let’s first start off with some motivation. Imagine we have a dataset of movies with their respective average rating from a streaming service. The rating can be any integer between 1 and 5. After creating a histogram with maptlotlib, we see that there are a few movies with an average rating of 6, which is well above the allowable range. This is most likely an error in data collection or parsing, where a variable is well beyond its range and treating it is essential to have accurate analysis. Here’s another example, where we see subscription dates in the future for a service. Inherently this doesn’t make any sense, as we cannot sign up for a service in the future, but these errors exist either due to technical or human error. We use the datetime package’s .date.today() function to get today’s date, and we filter the dataset by any subscription date higher than today’s date. We need to pay attention to the range of our data.</p>
</section>
<section id="how-to-deal-with-out-of-range-data" class="level3">
<h3 class="anchored" data-anchor-id="how-to-deal-with-out-of-range-data">How to deal with out of range data?</h3>
<p>There’s a variety of options to deal with out of range data. The simplest option is to drop the data. However, depending on the size of your out of range data, you could be losing out on essential information. As a rule of thumb, only drop data when a small proportion of your dataset is affected by out of range values, however you really need to understand your dataset before deciding to drop values. Another option would be setting custom minimums or maximums to your columns. We could also set the data to missing, and impute it, but we’ll take a look at how to deal with missing data in Chapter 3. We could also, dependent on the business assumptions behind our data, assign a custom value for any values of our data that go beyond a certain range.</p>
</section>
<section id="movie-example" class="level3">
<h3 class="anchored" data-anchor-id="movie-example">Movie example</h3>
<p>Let’s take a look at the movies example mentioned earlier. We first isolate the movies with ratings higher than 5. Now if these values are affect a small set of our data, we can drop them. We can drop them in two ways - we can either create a new filtered movies DataFrame where we only keep values of avg_rating lower or equal than to 5. Or drop the values by using the drop method. The drop method takes in as argument the row indices of movies for which the avg_rating is higher than 5. We set the inplace argument to True so that values are dropped in place and we don’t have to create a new column. We can make sure this is set in place using an assert statement that checks if the maximum of avg_rating is lower or equal than to 5. Depending on the assumptions behind our data, we can also change the out of range values to a hard limit. For example, here we’re setting any value of the avg_rating column in to 5 if it goes beyond it. We can do this using the dot-loc method, which returns all cells that fit a custom row and column index. It takes as first argument the row index, or here all instances of avg_rating above 5 and as second argument the column index, which is here the avg_rating column. Again, we can make sure that this change was done using an assert statement.</p>
</section>
<section id="date-range-example" class="level3">
<h3 class="anchored" data-anchor-id="date-range-example">Date range example</h3>
<p>Let’s take another look at the date range example mentioned earlier, where we had subscriptions happening in the future. We first look at the data types of the column with the dot-dtypes attribute. We can confirm that the subscription_date column is an object and not a date or datetime object. To compare a pandas object to a date, the first step is to convert it to another date. We do so by first converting it into a pandas datetime object with the to_datetime function from pandas, which takes in as an argument the column we want to convert. We then need to convert the datetime object into a date. This conversion is done by appending dt-dot-date to the code. Could we have converted from an object directly to a date, without the pandas datetime conversion in the middle? Yes! But we’d have had to provide information about the date’s format as a string, so it’s just as easy to do it this way. Now that the column is a date, we can treat it in a variety of ways. We first create a today_date variable using the datetime function date-dot-today, which allows us to store today’s date. We can then either drop the rows with exceeding dates similar to how we did in the average rating example, or replace exceeding values with today’s date. In both cases we can use the assert statement to verify our treatment went well, by comparing the maximum value in the subscription_date column. However, make sure to chain it with the dot-date method to return a date instead of a timestamp.</p>
</section>
</section>
<section id="exercise-1.2.1" class="level2">
<h2 class="anchored" data-anchor-id="exercise-1.2.1">Exercise 1.2.1</h2>
<section id="tire-size-constraints" class="level3">
<h3 class="anchored" data-anchor-id="tire-size-constraints">Tire size constraints</h3>
<p>In this lesson, you’re going to build on top of the work you’ve been doing with the <code>ride_sharing</code> DataFrame. You’ll be working with the tire_sizes column which contains data on each bike’s <code>tire size</code>.</p>
<p>Bicycle tire sizes could be either 26″, 27″ or 29″ and are here correctly stored as a categorical value. In an effort to cut maintenance costs, the ride sharing provider decided to set the maximum tire size to be 27″.</p>
<p>In this exercise, you will make sure the <code>tire_sizes</code> column has the correct range by first converting it to an integer, then setting and testing the new upper limit of 27″ for <code>tire sizes</code>.</p>
</section>
<section id="instructions-2" class="level3">
<h3 class="anchored" data-anchor-id="instructions-2">Instructions</h3>
<ul>
<li>Convert the tire_sizes column from category to <code>'int'</code>.</li>
<li>Use <code>.loc[]</code> to set all values of <code>tire_sizes</code> above 27 to 27.</li>
<li>Reconvert back <code>tire_sizes</code> to <code>'category'</code> from <code>int</code>.</li>
<li>Print the description of the <code>tire_sizes</code>.</li>
</ul>
</section>
</section>
<section id="exercise-1.2.2" class="level2">
<h2 class="anchored" data-anchor-id="exercise-1.2.2">Exercise 1.2.2</h2>
<section id="back-to-the-future" class="level3">
<h3 class="anchored" data-anchor-id="back-to-the-future">Back to the future</h3>
<p>A new update to the data pipeline feeding into the <code>ride_sharing</code> DataFrame has been updated to register each ride’s date. This information is stored in the <code>ride_date</code> column of the type <code>object</code>, which represents strings in <code>pandas</code>.</p>
<p>A bug was discovered which was relaying rides taken today as taken next year. To fix this, you will find all instances of the ride_date column that occur anytime in the future, and set the maximum possible value of this column to today’s date. Before doing so, you would need to convert <code>ride_date</code> to a <code>datetime</code> object.</p>
</section>
<section id="instructions-3" class="level3">
<h3 class="anchored" data-anchor-id="instructions-3">Instructions</h3>
<ul>
<li>Convert <code>ride_date</code> to a <code>datetime</code> object using <code>to_datetime()</code>, then convert the <code>datetime</code> object into a <code>date</code> and store it in <code>ride_dt</code> column.</li>
<li>Create the variable <code>today</code>, which stores today’s date by using the <code>dt.date.today()</code> function.</li>
<li>For all instances of <code>ride_dt</code> in the future, set them to today’s date.</li>
<li>Print the maximum date in the <code>ride_dt</code> column.</li>
</ul>
</section>
</section>
<section id="chapter-1.3-uniqueness-constraints" class="level2">
<h2 class="anchored" data-anchor-id="chapter-1.3-uniqueness-constraints">Chapter 1.3: Uniqueness constraints</h2>
<p>Hi and welcome to the final lesson of this chapter. Let’s discuss another common data cleaning problem, duplicate values.</p>
<section id="what-are-duplicate-values" class="level3">
<h3 class="anchored" data-anchor-id="what-are-duplicate-values">What are duplicate values?</h3>
<p>Duplicate values can be diagnosed when we have the same exact information repeated across multiple rows, for a some or all columns in our DataFrame. In this example DataFrame containing the names, address, height, and weight of individuals, the rows presented have identical values across all columns. In this one, there are duplicate values for all columns except the height column – which leads us to think it’s more likely a data entry error than an actual other person.</p>
</section>
<section id="why-do-they-happen" class="level3">
<h3 class="anchored" data-anchor-id="why-do-they-happen">Why do they happen?</h3>
<p>Apart from data entry and human errors alluded to in the previous slide, duplicate data can also arise because of bugs and design errors whether in business processes or data pipelines. However they oftenmost arise from the necessary act of joining and consolidating data from various resources, which could retain duplicate values.</p>
</section>
<section id="how-to-find-duplicate-values" class="level3">
<h3 class="anchored" data-anchor-id="how-to-find-duplicate-values">How to find duplicate values?</h3>
<p>Let’s first see how to find duplicate values. In this example, we’re working with a bigger version of the the height and weight data seen earlier in the video. We can find duplicates in a DataFrame by using the <code>.duplicated()</code> method. It returns a Series of boolean values that are True for duplicate values, and False for non-duplicated values. We can see exactly which rows are affected by using brackets as such. However, using <code>.duplicated()</code> without playing around with the arguments of the method can lead to misleading results, as all the columns are required to have duplicate values by default, with all duplicate values being marked as True except for the first occurrence. This limits our ability to properly diagnose what type of duplication we have, and how to effectively treat it. To properly calibrate how we go about finding duplicates, we will use 2 arguments from the <code>.duplicated()</code> method. The subset argument lets us set a list of column names to check for duplication. For example, it allows us to find duplicates for the first and last name columns only. The keep argument lets us keep the first occurrence of a duplicate value by setting it to the string first, the last occurrence of a duplicate value by setting it the string last, or keep all occurrences of duplicate values by setting it to False. In this example, we’re checking for duplicates across the first name, last name, and address variables, and we’re choosing to keep all duplicates. We see the following results – to get a better bird’s eye view of the duplicates, We sort the duplicate rows using the .sort_values method, choosing first_name to sort by. We find that there are four sets of duplicated rows, the first 2 being complete duplicates of each other across all columns, highlighted here in red. The other 2 being incomplete duplicates of each other highlighted here in blue with discrepancies across height and weight respectively.</p>
</section>
<section id="how-to-treat-duplicate-values" class="level3">
<h3 class="anchored" data-anchor-id="how-to-treat-duplicate-values">How to treat duplicate values?</h3>
<p>The complete duplicates can be treated easily. All that is required is to keep one of them only and discard the others. This can be done with the <code>.drop_duplicates()</code> method, which also takes in the same subset and keep arguments as in the <code>.duplicated()</code> method, as well as the inplace argument which drops the duplicated values directly inside the height_weight DataFrame. Here we are dropping complete duplicates only, so it’s not necessary nor advisable to set a subset, and since the keep argument takes in first as default, we can keep it as such. Note that we can also set it as last, but not as False as it would keep all duplicates. This leaves us with the other 2 sets of duplicates discussed earlier, which are the same for first_name, last_name and address, but contain discrepancies in height and weight. Apart from dropping rows with really small discrepancies, we can use a statistical measure to combine each set of duplicated values. For example, we can combine these two rows into one by computing the average mean between them, or the maximum, or other statistical measures, this is highly dependent on a common sense understanding of our data, and what type of data we have.</p>
<p>We can do this easily using the groupby method, which when chained with the agg method, lets you group by a set of common columns and return statistical values for specific columns when the aggregation is being performed. For example here, we created a dictionary called summaries, which instructs groupby to return the maximum of duplicated rows for the height column, and the mean duplicated rows for the weight column. We then group height_weight by the column names defined earlier, and chained it with the agg method, which takes in the summaries dictionary we created. We chain this entire line with the <code>.reset_index()</code> method, so that we can have numbered indices in the final output. We can verify that there are no more duplicate values by running the duplicated method again, and use brackets to output duplicate rows.</p>
</section>
</section>
<section id="exercise-1.3.1" class="level2">
<h2 class="anchored" data-anchor-id="exercise-1.3.1">Exercise 1.3.1</h2>
<section id="finding-duplicates" class="level3">
<h3 class="anchored" data-anchor-id="finding-duplicates">Finding duplicates</h3>
<p>A new update to the data pipeline feeding into <code>ride_sharing</code> has added the <code>bike_id column</code>, which represents a unique identifier for each ride.</p>
<p>The update however coincided with radically shorter average ride duration times and irregular user birth dates set in the future. Most importantly, the number of rides taken has increased by 20% overnight, leading you to think there might be both complete and incomplete duplicates in the <code>ride_sharing</code> DataFrame.</p>
<p>In this exercise, you will confirm this suspicion by finding those duplicates. A sample of <code>ride_sharing</code> is in your environment, as well as all the packages you’ve been working with thus far.</p>
</section>
<section id="instructions-4" class="level3">
<h3 class="anchored" data-anchor-id="instructions-4">Instructions</h3>
<ul>
<li>Find duplicated rows of <code>ride_id</code> in the <code>ride_sharing</code> DataFrame while setting <code>keep</code> to <code>False</code>.</li>
<li>Subset <code>ride_sharing</code> on <code>duplicates</code> and sort by <code>bike_id</code> and assign the results to <code>duplicated_rides</code>.</li>
<li>Print the <code>bike_id</code>, <code>duration</code> and <code>user_birth_year</code> columns of <code>duplicated_rides</code> in that order.</li>
</ul>
</section>
<section id="exercise-1.3.2" class="level3">
<h3 class="anchored" data-anchor-id="exercise-1.3.2">Exercise 1.3.2</h3>
<p><strong>Treating duplicates</strong></p>
<p>In the last exercise, you were able to verify that the new update feeding into <code>ride_sharing</code> contains a bug generating both complete and incomplete duplicated rows for some values of the <code>bike_id</code> column, with occasional discrepant values for the <code>user_birth_year</code> and <code>duration</code> columns.</p>
<p>In this exercise, you will be treating those duplicated rows by first dropping complete duplicates, and then merging the incomplete duplicate rows into one while keeping the average <code>duration</code>, and the minimum <code>user_birth_year</code> for each set of incomplete duplicate rows.</p>
<p><strong>Instructions</strong></p>
<ul>
<li>Drop complete duplicates in <code>ride_sharing</code> and store the results in <code>ride_dup</code>.</li>
<li>Create the <code>statistics</code> dictionary which holds <strong>min</strong>imum aggregation for <code>user_birth_year</code> and <strong>mean</strong> aggregation for <code>duration</code>.</li>
<li>Drop incomplete duplicates by grouping by <code>bike_id</code> and applying the aggregation in <code>statistics</code>.</li>
<li>Find duplicates again and run the <code>assert</code> statement to verify de-duplication.</li>
</ul>
<div id="367703ae" class="cell" data-execution_count="1">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Import the course packages</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> datetime <span class="im">as</span> dt</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> missingno <span class="im">as</span> msno</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> fuzzywuzzy</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> recordlinkage </span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Import the course dataset</span></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>ride_sharing <span class="op">=</span> pd.read_csv(<span class="st">'datasets/ride_sharing.mod.csv'</span>, index_col <span class="op">=</span> <span class="st">'Unnamed: 0'</span>)</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Print the information of ride_sharing</span></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(ride_sharing.info())</span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a><span class="co"># Print summary statistics of user_type column</span></span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(ride_sharing[<span class="st">'user_type'</span>].describe())</span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a><span class="co"># Convert user_type from integer to category</span></span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a>ride_sharing[<span class="st">'user_type_cat'</span>] <span class="op">=</span> ride_sharing[<span class="st">'user_type'</span>].astype(<span class="st">'category'</span>)</span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a><span class="co"># Write an assert statement confirming the change</span></span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a><span class="cf">assert</span> ride_sharing[<span class="st">'user_type_cat'</span>].dtype <span class="op">==</span> <span class="st">'category'</span></span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a><span class="co"># Print new summary statistics </span></span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(ride_sharing[<span class="st">'user_type_cat'</span>].describe())</span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a><span class="co"># Strip duration of minutes</span></span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a>ride_sharing[<span class="st">'duration_trim'</span>] <span class="op">=</span> ride_sharing[<span class="st">'duration'</span>].<span class="bu">str</span>.strip(<span class="st">'minutes'</span>)</span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-31"><a href="#cb1-31" aria-hidden="true" tabindex="-1"></a><span class="co"># Convert duration to integer</span></span>
<span id="cb1-32"><a href="#cb1-32" aria-hidden="true" tabindex="-1"></a>ride_sharing[<span class="st">'duration_time'</span>] <span class="op">=</span> ride_sharing[<span class="st">'duration_trim'</span>].astype(<span class="st">'int'</span>)</span>
<span id="cb1-33"><a href="#cb1-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-34"><a href="#cb1-34" aria-hidden="true" tabindex="-1"></a><span class="co"># Write an assert statement making sure of conversion</span></span>
<span id="cb1-35"><a href="#cb1-35" aria-hidden="true" tabindex="-1"></a><span class="cf">assert</span> ride_sharing[<span class="st">'duration_time'</span>].dtype <span class="op">==</span> <span class="st">'int'</span></span>
<span id="cb1-36"><a href="#cb1-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-37"><a href="#cb1-37" aria-hidden="true" tabindex="-1"></a><span class="co"># Print formed columns and calculate average ride duration </span></span>
<span id="cb1-38"><a href="#cb1-38" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(ride_sharing[[<span class="st">'duration'</span>,<span class="st">'duration_trim'</span>,<span class="st">'duration_time'</span>]])</span>
<span id="cb1-39"><a href="#cb1-39" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(ride_sharing[<span class="st">'duration_time'</span>].mean())</span>
<span id="cb1-40"><a href="#cb1-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-41"><a href="#cb1-41" aria-hidden="true" tabindex="-1"></a><span class="co"># 11 minutes is really not bad for an average ride duration in a city like San-Francisco.</span></span>
<span id="cb1-42"><a href="#cb1-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-43"><a href="#cb1-43" aria-hidden="true" tabindex="-1"></a><span class="co"># Convert tire_sizes to integer</span></span>
<span id="cb1-44"><a href="#cb1-44" aria-hidden="true" tabindex="-1"></a>ride_sharing[<span class="st">'tire_sizes'</span>] <span class="op">=</span> ride_sharing[<span class="st">'tire_sizes'</span>].astype(<span class="st">'int'</span>)</span>
<span id="cb1-45"><a href="#cb1-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-46"><a href="#cb1-46" aria-hidden="true" tabindex="-1"></a><span class="co"># Set all values above 27 to 27</span></span>
<span id="cb1-47"><a href="#cb1-47" aria-hidden="true" tabindex="-1"></a>ride_sharing.loc[ride_sharing[<span class="st">'tire_sizes'</span>] <span class="op">&gt;</span> <span class="dv">27</span>, <span class="st">'tire_sizes'</span>] <span class="op">=</span> <span class="dv">27</span></span>
<span id="cb1-48"><a href="#cb1-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-49"><a href="#cb1-49" aria-hidden="true" tabindex="-1"></a><span class="co"># Reconvert tire_sizes back to categorical</span></span>
<span id="cb1-50"><a href="#cb1-50" aria-hidden="true" tabindex="-1"></a>ride_sharing[<span class="st">'tire_sizes'</span>] <span class="op">=</span> ride_sharing[<span class="st">'tire_sizes'</span>].astype(<span class="st">'category'</span>)</span>
<span id="cb1-51"><a href="#cb1-51" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-52"><a href="#cb1-52" aria-hidden="true" tabindex="-1"></a><span class="co"># Print tire size description</span></span>
<span id="cb1-53"><a href="#cb1-53" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(ride_sharing[<span class="st">'tire_sizes'</span>].describe())</span>
<span id="cb1-54"><a href="#cb1-54" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-55"><a href="#cb1-55" aria-hidden="true" tabindex="-1"></a><span class="co"># Convert ride_date to date</span></span>
<span id="cb1-56"><a href="#cb1-56" aria-hidden="true" tabindex="-1"></a>ride_sharing[<span class="st">'ride_dt'</span>] <span class="op">=</span> pd.to_datetime(ride_sharing[<span class="st">'ride_date'</span>]).dt.date</span>
<span id="cb1-57"><a href="#cb1-57" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-58"><a href="#cb1-58" aria-hidden="true" tabindex="-1"></a><span class="co"># Save today's date</span></span>
<span id="cb1-59"><a href="#cb1-59" aria-hidden="true" tabindex="-1"></a>today <span class="op">=</span> dt.date.today()</span>
<span id="cb1-60"><a href="#cb1-60" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-61"><a href="#cb1-61" aria-hidden="true" tabindex="-1"></a><span class="co"># Set all in the future to today's date</span></span>
<span id="cb1-62"><a href="#cb1-62" aria-hidden="true" tabindex="-1"></a>ride_sharing.loc[ride_sharing[<span class="st">'ride_dt'</span>] <span class="op">&gt;</span> today, <span class="st">'ride_dt'</span>] <span class="op">=</span> today</span>
<span id="cb1-63"><a href="#cb1-63" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-64"><a href="#cb1-64" aria-hidden="true" tabindex="-1"></a><span class="co"># Print maximum of ride_dt column</span></span>
<span id="cb1-65"><a href="#cb1-65" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(ride_sharing[<span class="st">'ride_dt'</span>].<span class="bu">max</span>())</span>
<span id="cb1-66"><a href="#cb1-66" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-67"><a href="#cb1-67" aria-hidden="true" tabindex="-1"></a><span class="co"># Find duplicates</span></span>
<span id="cb1-68"><a href="#cb1-68" aria-hidden="true" tabindex="-1"></a>duplicates <span class="op">=</span> ride_sharing.duplicated(<span class="st">'bike_id'</span>, keep<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb1-69"><a href="#cb1-69" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-70"><a href="#cb1-70" aria-hidden="true" tabindex="-1"></a><span class="co"># Sort your duplicated rides</span></span>
<span id="cb1-71"><a href="#cb1-71" aria-hidden="true" tabindex="-1"></a>duplicated_rides <span class="op">=</span> ride_sharing[duplicates].sort_values(<span class="st">'bike_id'</span>)</span>
<span id="cb1-72"><a href="#cb1-72" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-73"><a href="#cb1-73" aria-hidden="true" tabindex="-1"></a><span class="co"># Print relevant columns of duplicated_rides</span></span>
<span id="cb1-74"><a href="#cb1-74" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(duplicated_rides[[<span class="st">'bike_id'</span>,<span class="st">'duration'</span>,<span class="st">'user_birth_year'</span>]])</span>
<span id="cb1-75"><a href="#cb1-75" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-76"><a href="#cb1-76" aria-hidden="true" tabindex="-1"></a><span class="co"># Drop complete duplicates from ride_sharing</span></span>
<span id="cb1-77"><a href="#cb1-77" aria-hidden="true" tabindex="-1"></a>ride_dup <span class="op">=</span> ride_sharing.drop_duplicates()</span>
<span id="cb1-78"><a href="#cb1-78" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-79"><a href="#cb1-79" aria-hidden="true" tabindex="-1"></a><span class="co"># Create statistics dictionary for aggregation function</span></span>
<span id="cb1-80"><a href="#cb1-80" aria-hidden="true" tabindex="-1"></a>statistics <span class="op">=</span> {<span class="st">'user_birth_year'</span>: <span class="st">'min'</span>, <span class="st">'duration_time'</span>: <span class="st">'mean'</span>}</span>
<span id="cb1-81"><a href="#cb1-81" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-82"><a href="#cb1-82" aria-hidden="true" tabindex="-1"></a><span class="co"># Group by bike_id and compute new statistics</span></span>
<span id="cb1-83"><a href="#cb1-83" aria-hidden="true" tabindex="-1"></a>ride_unique <span class="op">=</span> ride_dup.groupby(by <span class="op">=</span> <span class="st">'bike_id'</span>).agg(statistics).reset_index()</span>
<span id="cb1-84"><a href="#cb1-84" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-85"><a href="#cb1-85" aria-hidden="true" tabindex="-1"></a><span class="co"># Find duplicated values again</span></span>
<span id="cb1-86"><a href="#cb1-86" aria-hidden="true" tabindex="-1"></a>duplicates <span class="op">=</span> ride_unique.duplicated(subset <span class="op">=</span> <span class="st">'bike_id'</span>, keep <span class="op">=</span> <span class="va">False</span>)</span>
<span id="cb1-87"><a href="#cb1-87" aria-hidden="true" tabindex="-1"></a>duplicated_rides <span class="op">=</span> ride_unique[duplicates <span class="op">==</span> <span class="va">True</span>]</span>
<span id="cb1-88"><a href="#cb1-88" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-89"><a href="#cb1-89" aria-hidden="true" tabindex="-1"></a><span class="co"># Assert duplicates are processed</span></span>
<span id="cb1-90"><a href="#cb1-90" aria-hidden="true" tabindex="-1"></a><span class="cf">assert</span> duplicated_rides.shape[<span class="dv">0</span>] <span class="op">==</span> <span class="dv">0</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>&lt;class 'pandas.core.frame.DataFrame'&gt;
Index: 25760 entries, 0 to 25759
Data columns (total 11 columns):
 #   Column           Non-Null Count  Dtype 
---  ------           --------------  ----- 
 0   duration         25760 non-null  object
 1   station_A_id     25760 non-null  int64 
 2   station_A_name   25760 non-null  object
 3   station_B_id     25760 non-null  int64 
 4   station_B_name   25760 non-null  object
 5   bike_id          25760 non-null  int64 
 6   user_type        25760 non-null  int64 
 7   user_birth_year  25760 non-null  int64 
 8   user_gender      25760 non-null  object
 9   tire_sizes       25760 non-null  int64 
 10  ride_date        25760 non-null  object
dtypes: int64(6), object(5)
memory usage: 2.4+ MB
None
count    25760.000000
mean         2.008385
std          0.704541
min          1.000000
25%          2.000000
50%          2.000000
75%          3.000000
max          3.000000
Name: user_type, dtype: float64
count     25760
unique        3
top           2
freq      12972
Name: user_type_cat, dtype: int64
         duration duration_trim  duration_time
0      12 minutes           12              12
1      24 minutes           24              24
2       8 minutes            8               8
3       4 minutes            4               4
4      11 minutes           11              11
...           ...           ...            ...
25755  11 minutes           11              11
25756  10 minutes           10              10
25757  14 minutes           14              14
25758  14 minutes           14              14
25759  29 minutes           29              29

[25760 rows x 3 columns]
11.389052795031056
count     25760
unique        2
top          27
freq      17173
Name: tire_sizes, dtype: int64
2024-10-04
       bike_id    duration  user_birth_year
3638        11  12 minutes             1988
6088        11   5 minutes             1985
10857       11   4 minutes             1987
10045       27  13 minutes             1989
16104       27  10 minutes             1970
...        ...         ...              ...
8812      6638  10 minutes             1986
6815      6638   5 minutes             1995
8456      6638   7 minutes             1983
8300      6638   6 minutes             1962
8380      6638   8 minutes             1984

[25717 rows x 3 columns]</code></pre>
</div>
</div>
</section>
</section>
<section id="chapter-2-text-and-categorical-data-problems" class="level2">
<h2 class="anchored" data-anchor-id="chapter-2-text-and-categorical-data-problems">CHAPTER 2: TEXT AND CATEGORICAL DATA PROBLEMS</h2>
</section>
<section id="chapter-2.1-membership-constraints" class="level2">
<h2 class="anchored" data-anchor-id="chapter-2.1-membership-constraints">Chapter 2.1: Membership constraints</h2>
<p>Fantastic work on Chapter 1! You’re now equipped to treat more complex, and specific data cleaning problems.</p>
<section id="in-this-chapter" class="level3">
<h3 class="anchored" data-anchor-id="in-this-chapter">In this chapter</h3>
<p>In this chapter, we’re going to take a look at common data problems with text and categorical data, so let’s get started.</p>
</section>
<section id="categories-and-membership-constraints" class="level3">
<h3 class="anchored" data-anchor-id="categories-and-membership-constraints">Categories and membership constraints</h3>
<p>In this lesson, we’ll focus on categorical variables. As discussed early in chapter 1, categorical data represent variables that represent predefined finite set of categories. Examples of this range from marriage status, household income categories, loan status and others. To run machine learning models on categorical data, they are often coded as numbers. Since categorical data represent a predefined set of categories, they can’t have values that go beyond these predefined categories.</p>
</section>
<section id="why-could-we-have-these-problems" class="level3">
<h3 class="anchored" data-anchor-id="why-could-we-have-these-problems">Why could we have these problems?</h3>
<p>We can have inconsistencies in our categorical data for a variety of reasons. This could be due to data entry issues with free text vs dropdown fields, data parsing errors and other types of errors.</p>
</section>
<section id="how-do-we-treat-these-problems" class="level3">
<h3 class="anchored" data-anchor-id="how-do-we-treat-these-problems">How do we treat these problems?</h3>
<p>There’s a variety of ways we can treat these, with increasingly specific solutions for different types of inconsistencies. Most simply, we can drop the rows with incorrect categories. We can attempt remapping incorrect categories to correct ones, and more. We’ll see a variety of ways of dealing with this throughout the chapter and the course, but for now we’ll just focus on dropping data. An example Let’s first look at an example. Here’s a DataFrame named study_data containing a list of first names, birth dates, and blood types. Additionally, a DataFrame named categories, containing the correct possible categories for the blood type column has been created as well. Notice the inconsistency here? There’s definitely no blood type named Z+. Luckily, the categories DataFrame will help us systematically spot all rows with these inconsistencies. It’s always good practice to keep a log of all possible values of your categorical data, as it will make dealing with these types of inconsistencies way easier.</p>
</section>
<section id="a-note-on-joins" class="level3">
<h3 class="anchored" data-anchor-id="a-note-on-joins">A note on joins</h3>
<p>Now before moving on to dealing with these inconsistent values, let’s have a brief reminder on joins. The two main types of joins we care about here are anti joins and inner joins. We join DataFrames on common columns between them. Anti joins, take in two DataFrames A and B, and return data from one DataFrame that is not contained in another. In this example, we are performing a left anti join of A and B, and are returning the columns of DataFrames A and B for values only found in A of the common column between them being joined on. Inner joins, return only the data that is contained in both DataFrames. For example, an inner join of A and B, would return columns from both DataFrames for values only found in A and B, of the common column between them being joined on.</p>
</section>
<section id="a-left-anti-join-on-blood-types" class="level3">
<h3 class="anchored" data-anchor-id="a-left-anti-join-on-blood-types">A left anti join on blood types</h3>
<p>In our example, an left anti join essentially returns all the data in study data with inconsistent blood types,</p>
</section>
<section id="an-inner-join-on-blood-types" class="level3">
<h3 class="anchored" data-anchor-id="an-inner-join-on-blood-types">An inner join on blood types</h3>
<p>and an inner join returns all the rows containing consistent blood types signs.</p>
</section>
<section id="finding-inconsistent-categories" class="level3">
<h3 class="anchored" data-anchor-id="finding-inconsistent-categories">Finding inconsistent categories**</h3>
<p>Now let’s see how to do that in Python. We first get all inconsistent categories in the blood_type column of the study_data DataFrame. We do that by creating a set out of the blood_type column which stores its unique values, and use the difference method which takes in as argument the blood_type column from the categories DataFrame. This returns all the categories in blood_type that are not in categories. We then find the inconsistent rows by finding all the rows of the blood_type columns that are equal to inconsistent categories by using the isin method, this returns a series of boolean values that are True for inconsistent rows and False for consistent ones. We then subset the study_data DataFrame based on these boolean values, and voila we have our inconsistent data.</p>
</section>
<section id="dropping-inconsistent-categories" class="level3">
<h3 class="anchored" data-anchor-id="dropping-inconsistent-categories">Dropping inconsistent categories</h3>
<p>To drop inconsistent rows and keep ones that are only consistent. We just use the tilde symbol while subsetting which returns everything except inconsistent rows</p>
</section>
</section>
<section id="exercise-2.1" class="level2">
<h2 class="anchored" data-anchor-id="exercise-2.1">Exercise 2.1</h2>
<section id="finding-consistency" class="level3">
<h3 class="anchored" data-anchor-id="finding-consistency">Finding consistency</h3>
<p>In this exercise and throughout this chapter, you’ll be working with the <code>airlines</code> DataFrame which contains survey responses on the San Francisco Airport from airline customers.</p>
<p>The DataFrame contains flight metadata such as the airline, the destination, waiting times as well as answers to key questions regarding cleanliness, safety, and satisfaction. Another DataFrame named <code>categories</code> was created, containing all correct possible values for the survey columns.</p>
<p>In this exercise, you will use both of these DataFrames to find survey answers with inconsistent values, and drop them, effectively performing an outer and inner join on both these DataFrames as seen in the video exercise.</p>
</section>
<section id="instructions-5" class="level3">
<h3 class="anchored" data-anchor-id="instructions-5">Instructions</h3>
<ul>
<li>Print the <code>categories</code> DataFrame and take a close look at all possible correct categories of the survey columns.</li>
<li>Print the unique values of the survey columns in <code>airlines</code> using the <code>.unique()</code> method.</li>
<li>Create a set out of the <code>cleanliness</code> column in <code>airlines</code> using <code>set()</code> and find the inconsistent category by finding the <strong>difference</strong> in the <code>cleanliness</code> column of <code>categories</code>.</li>
<li>Find rows of <code>airlines</code> with a <code>cleanliness</code> value not in <code>categories</code> and print the output.</li>
<li>Print the rows with the consistent categories of <code>cleanliness</code> only.</li>
</ul>
</section>
</section>
<section id="chapter-2.2-categorical-variables" class="level2">
<h2 class="anchored" data-anchor-id="chapter-2.2-categorical-variables">Chapter 2.2: Categorical variables</h2>
<p>Awesome work on the last lesson. Now let’s discuss other types of problems that could affect categorical variables.</p>
<section id="what-type-of-errors-could-we-have" class="level3">
<h3 class="anchored" data-anchor-id="what-type-of-errors-could-we-have">What type of errors could we have?</h3>
<p>In the last lesson, we saw how categorical data has a value membership constraint, where columns need to have a predefined set of values. However, this is not the only set of problems we may encounter. When cleaning categorical data, some of the problems we may encounter include value inconsistency, the presence of too many categories that could be collapsed into one, and making sure data is of the right type.</p>
</section>
<section id="value-consistency" class="level3">
<h3 class="anchored" data-anchor-id="value-consistency">Value consistency</h3>
<p>Let’s start with making sure our categorical data is consistent. A common categorical data problem is having values that slightly differ because of capitalization. Not treating this could lead to misleading results when we decide to analyze our data, for example, let’s assume we’re working with a demographics dataset, and we have a marriage status column with inconsistent capitalization. Here’s what counting the number of married people in the marriage_status Series would look like. Note that the <code>.value_counts()</code> methods works on Series only. For a DataFrame, we can groupby the column and use the <code>.count()</code> method. To deal with this, we can either capitalize or lowercase the marriage_status column. This can be done with the <code>str.upper()</code> or <code>.lower()</code> functions respectively. Another common problem with categorical values are leading or trailing spaces. For example, imagine the same demographics DataFrame containing values with leading spaces. Here’s what the counts of married vs unmarried people would look like. Note that there is a married category with a trailing space on the right, which makes it hard to spot on the output, as opposed to unmarried. To remove leading spaces, we can use the <code>.str.strip()</code> method which when given no input, strips all leading and trailing white spaces.</p>
</section>
<section id="collapsing-data-into-categories" class="level3">
<h3 class="anchored" data-anchor-id="collapsing-data-into-categories">Collapsing data into categories</h3>
<p>Sometimes, we may want to create categories out of our data, such as creating household income groups from income data. To create categories out of data, let’s use the example of creating an income group column in the demographics DataFrame. We can do this in 2 ways. The first method utilizes the qcut function from pandas, which automatically divides our data based on its distribution into the number of categories we set in the q argument, we created the category names in the group_names list and fed it to the labels argument, returning the following. Notice that the first row actually misrepresents the actual income of the income group, as we didn’t instruct qcut where our ranges actually lie. We can do this with the cut function instead, which lets us define category cutoff ranges with the bins argument. It takes in a list of cutoff points for each category, with the final one being infinity represented with <code>np.inf()</code>. From the output, we can see this is much more correct. Sometimes, we may want to reduce the amount of categories we have in our data. Let’s move on to mapping categories to fewer ones. For example, assume we have a column containing the operating system of different devices, and contains these unique values. Say we want to collapse these categories into 2, DesktopOS, and MobileOS. We can do this using the replace method. It takes in a dictionary that maps each existing category to the category name you desire. In this case, this is the mapping dictionary. A quick print of the unique values of operating system shows the mapping has been complete.</p>
</section>
</section>
<section id="exercise-2.2.1" class="level2">
<h2 class="anchored" data-anchor-id="exercise-2.2.1">Exercise 2.2.1</h2>
<section id="inconsistent-categories" class="level3">
<h3 class="anchored" data-anchor-id="inconsistent-categories">Inconsistent categories</h3>
<p>In this exercise, you’ll be revisiting the <code>airlines</code> DataFrame from the previous lesson.</p>
<p>As a reminder, the DataFrame contains flight metadata such as the airline, the destination, waiting times as well as answers to key questions regarding cleanliness, safety, and satisfaction on the San Francisco Airport.</p>
<p>In this exercise, you will examine two categorical columns from this DataFrame, <code>dest_region</code> and <code>dest_size</code> respectively, assess how to address them and make sure that they are cleaned and ready for analysis.</p>
</section>
<section id="instructions-6" class="level3">
<h3 class="anchored" data-anchor-id="instructions-6">Instructions</h3>
<ul>
<li>Print the unique values in <code>dest_region</code> and <code>dest_size</code> respectively.</li>
<li>Change the capitalization of all values of <code>dest_region</code> to lowercase.</li>
<li>Replace the <code>'eur'</code> with <code>'europe'</code> in <code>dest_region</code> using the <code>.replace()</code> method.</li>
<li>Strip white spaces from the <code>dest_size</code> column using the <code>.strip()</code> method.</li>
<li>Verify that the changes have been into effect by printing the unique values of the columns using <code>.unique()</code>.</li>
</ul>
</section>
</section>
<section id="exercise-2.2.2" class="level2">
<h2 class="anchored" data-anchor-id="exercise-2.2.2">Exercise 2.2.2</h2>
<section id="remapping-categories" class="level3">
<h3 class="anchored" data-anchor-id="remapping-categories">Remapping categories</h3>
<p>To better understand survey respondents from <code>airlines</code>, you want to find out if there is a relationship between certain responses and the day of the week and wait time at the gate.</p>
<p>The <code>airlines</code> DataFrame contains the <code>day</code> and <code>wait_min</code> columns, which are categorical and numerical respectively. The <code>day</code> column contains the exact day a flight took place, and <code>wait_min</code> contains the amount of minutes it took travelers to wait at the gate. To make your analysis easier, you want to create two new categorical variables:</p>
<p><code>wait_type</code>: <code>'short'</code> for 0-60 min, <code>'medium'</code> for 60-180 and <code>long</code> for 180+</p>
<p><code>day_week</code>: <code>'weekday'</code> if day is in the weekday, <code>'weekend'</code> if day is in the weekend.</p>
</section>
<section id="instructions-7" class="level3">
<h3 class="anchored" data-anchor-id="instructions-7">Instructions</h3>
<ul>
<li>Create the ranges and labels for the <code>wait_type</code> column mentioned in the description.</li>
<li>Create the <code>wait_type</code> column by from <code>wait_min</code> by using <code>pd.cut()</code>, while inputting <code>label_ranges</code> and <code>label_names</code> in the correct arguments.</li>
<li>Create the <code>mapping</code> dictionary mapping weekdays to <code>'weekday'</code> and weekend days to <code>'weekend'</code>.</li>
<li>Create the <code>day_week</code> column by using <code>.replace()</code>.</li>
</ul>
</section>
</section>
<section id="chapter-2.3-cleaning-text-data" class="level2">
<h2 class="anchored" data-anchor-id="chapter-2.3-cleaning-text-data">Chapter 2.3: Cleaning text data</h2>
<p>Good job on the previous lesson. In the final lesson of this chapter, we’ll talk about text data and regular expressions.</p>
<section id="what-is-text-data" class="level3">
<h3 class="anchored" data-anchor-id="what-is-text-data">What is text data?</h3>
<p>Text data is one of the most common types of data types. Examples of it range from names, phone numbers, addresses, emails and more. Common text data problems include handling inconsistencies, making sure text data is of a certain length, typos and others.</p>
</section>
<section id="example" class="level3">
<h3 class="anchored" data-anchor-id="example">Example</h3>
<p>Let’s take a look at the following example. Here’s a DataFrame named phones containing the full name and phone numbers of individuals. Both are string columns. Notice the phone number column. We can see that there are phone number values, that begin with 00 or +. We also see that there is one entry where the phone number is 4 digits, which is non-existent. Furthermore, we can see that there are dashes across the phone number column. If we wanted to feed these phone numbers into an automated call system, or create a report discussing the distribution of users by area code, we couldn’t really do so without uniform phone numbers. Ideally, we’d want to the phone number column as such. Where all phone numbers are aligned to begin with 00, where any number below the 10 digit value is replaced with NaN to represent a missing value, and where all dashes have been removed. Let’s see how that’s done!</p>
</section>
<section id="fixing-the-phone-number-column" class="level3">
<h3 class="anchored" data-anchor-id="fixing-the-phone-number-column">Fixing the phone number column</h3>
<p>Let’s first begin by replacing the plus sign with 00, to do this, we use the dot str dot replace method which takes in two values, the string being replaced, which is in this case the plus sign and the string to replace it with which is in this case 00. We can see that the column has been updated. We use the same exact technique to remove the dashes, by replacing the dash symbol with an empty string. Now finally we’re going to replace all phone numbers below 10 digits to NaN. We can do this by chaining the Phone number column with the dot str dot len method, which returns the string length of each row in the column. We can then use the dot loc method, to index rows where digits is below 10, and replace the value of Phone number with numpy’s nan object, which is here imported as np. We can also write assert statements to test whether the phone number column has a specific length,and whether it contains the symbols we removed. The first assert statement tests that the minimum length of the strings in the phone number column, found through str dot len, is bigger than or equal to 10. In the second assert statement, we use the str dot contains method to test whether the phone number column contains a specific pattern. It returns a series of booleans for that are True for matches and False for non-matches. We set the pattern plus bar pipe minus, the bar pipe here is basically an or statement, so we’re trying to find matches for either symbols. We chain it with the any method which returns True if any element in the output of our dot-str-contains is True, and test whether the it returns False.</p>
</section>
<section id="but-what-about-more-complicated-examples" class="level3">
<h3 class="anchored" data-anchor-id="but-what-about-more-complicated-examples">But what about more complicated examples?</h3>
<p>But what about more complicated examples? How can we clean a phone number column that looks like this for example? Where phone numbers can contain a range of symbols from plus signs, dashes, parenthesis and maybe more. This is where regular expressions come in. Regular expressions give us the ability to search for any pattern in text data, like only digits for example. They are like control + find in your browser, but way more dynamic and robust.</p>
</section>
<section id="regular-expressions-in-action" class="level3">
<h3 class="anchored" data-anchor-id="regular-expressions-in-action">Regular expressions in action</h3>
<p>Let’s a look at this example. Here we are attempting to only extract digits from the phone number column. To do this, we use the dot str dot replace method with the pattern we want to replace with an empty string. Notice the pattern fed into the method. This is essentially us telling pandas to replace anything that is not a digit with nothing. We won’t get into the specifics of regular expressions, and how to construct them, but they are immensely useful for difficult string cleaning tasks, so make sure to check out DataCamp’s course library on regular expressions.</p>
</section>
</section>
<section id="exercise-2.3.1" class="level2">
<h2 class="anchored" data-anchor-id="exercise-2.3.1">Exercise 2.3.1</h2>
<section id="removing-titles-and-taking-names" class="level3">
<h3 class="anchored" data-anchor-id="removing-titles-and-taking-names">Removing titles and taking names</h3>
<p>While collecting survey respondent metadata in the <code>airlines</code> DataFrame, the full name of respondents was saved in the <code>full_name</code> column. However upon closer inspection, you found that a lot of the different names are prefixed by honorifics such as <code>"Dr."</code>, <code>"Mr."</code>, <code>"Ms."</code> and <code>"Miss"</code></p>
<p>Your ultimate objective is to create two new columns named <code>first_name</code> and <code>last_name</code>, containing the first and last names of respondents respectively. Before doing so however, you need to remove honorifics.</p>
</section>
<section id="instructions-8" class="level3">
<h3 class="anchored" data-anchor-id="instructions-8">Instructions</h3>
<ul>
<li>Remove <code>"Dr."</code>, <code>"Mr."</code>, <code>"Miss"</code> and <code>"Ms."</code> from <code>full_name</code> by replacing them with an empty string <code>""</code> in that order.</li>
<li>Run the <code>assert</code> statement using <code>.str.contains()</code> that tests whether <code>full_name</code> still contains any of the honorifics.</li>
</ul>
</section>
</section>
<section id="exercise-2.3.2" class="level2">
<h2 class="anchored" data-anchor-id="exercise-2.3.2">Exercise 2.3.2</h2>
<section id="keeping-it-descriptive" class="level3">
<h3 class="anchored" data-anchor-id="keeping-it-descriptive">Keeping it descriptive</h3>
<p>To further understand travelers’ experiences in the San Francisco Airport, the quality assurance department sent out a qualitative questionnaire to all travelers who gave the airport the worst score on all possible categories. The objective behind this questionnaire is to identify common patterns in what travelers are saying about the airport.</p>
<p>Their response is stored in the <code>survey_response</code> column. Upon a closer look, you realized a few of the answers gave the shortest possible character amount without much substance. In this exercise, you will isolate the responses with a character count higher than <em>40</em> , and make sure your new DataFrame contains responses with <em>40</em> characters or more using an <code>assert</code> statement.</p>
</section>
<section id="instructions-9" class="level3">
<h3 class="anchored" data-anchor-id="instructions-9">Instructions</h3>
<ul>
<li>Using the <code>airlines</code> DataFrame, store the length of each instance in the <code>survey_response</code> column in <code>resp_length</code> by using <code>.str.len()</code>.</li>
<li>Isolate the rows of <code>airlines</code> with <code>resp_length</code> higher than <code>40</code>.</li>
<li>Assert that the smallest <code>survey_response</code> length in <code>airlines_survey</code> is now bigger than <code>40</code>.</li>
</ul>
<div id="638adbda" class="cell" data-execution_count="2">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Import the course packages</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> datetime <span class="im">as</span> dt</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> missingno <span class="im">as</span> msno</span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> fuzzywuzzy</span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> recordlinkage </span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a>categories <span class="op">=</span> {<span class="st">'cleanliness'</span>: [<span class="st">'Clean'</span>, <span class="st">'Average'</span>, <span class="st">'Somewhat clean'</span>, <span class="st">'Somewhat dirty'</span>, <span class="st">'Dirty'</span>],</span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a>             <span class="st">'safety'</span>: [<span class="st">'Neutral'</span>, <span class="st">'Very safe'</span>, <span class="st">'Somewhat safe'</span>, <span class="st">'Very unsafe'</span>, <span class="st">'Somewhat unsafe'</span>],</span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a>             <span class="st">'satisfaction'</span>: [<span class="st">'Very satisfied'</span>, <span class="st">'Neutral'</span>, <span class="st">'Somewhat satisfied'</span>, <span class="st">'Somewhat unsatisfied'</span>, <span class="st">'Very unsatisfied'</span>]}</span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a>categories <span class="op">=</span> pd.DataFrame(categories)</span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true" tabindex="-1"></a><span class="co"># Import the course dataset</span></span>
<span id="cb3-17"><a href="#cb3-17" aria-hidden="true" tabindex="-1"></a>airlines <span class="op">=</span> pd.read_csv(<span class="st">'datasets/airlines.mod.csv'</span>,  index_col <span class="op">=</span> <span class="st">'Unnamed: 0'</span>)</span>
<span id="cb3-18"><a href="#cb3-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-19"><a href="#cb3-19" aria-hidden="true" tabindex="-1"></a><span class="co"># Print unique values of survey columns in airlines</span></span>
<span id="cb3-20"><a href="#cb3-20" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Cleanliness: '</span>, airlines[<span class="st">'cleanliness'</span>].unique(), <span class="st">"</span><span class="ch">\n</span><span class="st">"</span>)</span>
<span id="cb3-21"><a href="#cb3-21" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Safety: '</span>, airlines[<span class="st">'safety'</span>].unique(), <span class="st">"</span><span class="ch">\n</span><span class="st">"</span>)</span>
<span id="cb3-22"><a href="#cb3-22" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Satisfaction: '</span>, airlines[<span class="st">'satisfaction'</span>].unique(), <span class="st">"</span><span class="ch">\n</span><span class="st">"</span>)</span>
<span id="cb3-23"><a href="#cb3-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-24"><a href="#cb3-24" aria-hidden="true" tabindex="-1"></a><span class="co"># Find the cleanliness category in airlines not in categories DataFrame</span></span>
<span id="cb3-25"><a href="#cb3-25" aria-hidden="true" tabindex="-1"></a>cat_clean <span class="op">=</span> <span class="bu">set</span>(airlines[<span class="st">'cleanliness'</span>]).difference(categories[<span class="st">'cleanliness'</span>])</span>
<span id="cb3-26"><a href="#cb3-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-27"><a href="#cb3-27" aria-hidden="true" tabindex="-1"></a><span class="co"># Find rows with that category</span></span>
<span id="cb3-28"><a href="#cb3-28" aria-hidden="true" tabindex="-1"></a>cat_clean_rows <span class="op">=</span> airlines[<span class="st">'cleanliness'</span>].isin(cat_clean)</span>
<span id="cb3-29"><a href="#cb3-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-30"><a href="#cb3-30" aria-hidden="true" tabindex="-1"></a><span class="co"># Print rows with inconsistent category</span></span>
<span id="cb3-31"><a href="#cb3-31" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(airlines[cat_clean_rows])</span>
<span id="cb3-32"><a href="#cb3-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-33"><a href="#cb3-33" aria-hidden="true" tabindex="-1"></a><span class="co"># Print rows with consistent categories only using Tilde symbol (~)</span></span>
<span id="cb3-34"><a href="#cb3-34" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(airlines[<span class="op">~</span>cat_clean_rows])</span>
<span id="cb3-35"><a href="#cb3-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-36"><a href="#cb3-36" aria-hidden="true" tabindex="-1"></a><span class="co"># Print unique values of both columns</span></span>
<span id="cb3-37"><a href="#cb3-37" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(airlines[<span class="st">'dest_region'</span>].unique())</span>
<span id="cb3-38"><a href="#cb3-38" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(airlines[<span class="st">'dest_size'</span>].unique())</span>
<span id="cb3-39"><a href="#cb3-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-40"><a href="#cb3-40" aria-hidden="true" tabindex="-1"></a><span class="co"># Lower dest_region column and then replace "eur" with "europe"</span></span>
<span id="cb3-41"><a href="#cb3-41" aria-hidden="true" tabindex="-1"></a>airlines[<span class="st">'dest_region'</span>] <span class="op">=</span> airlines[<span class="st">'dest_region'</span>].<span class="bu">str</span>.lower()</span>
<span id="cb3-42"><a href="#cb3-42" aria-hidden="true" tabindex="-1"></a>airlines[<span class="st">'dest_region'</span>] <span class="op">=</span> airlines[<span class="st">'dest_region'</span>].replace({<span class="st">'eur'</span>:<span class="st">'europe'</span>})</span>
<span id="cb3-43"><a href="#cb3-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-44"><a href="#cb3-44" aria-hidden="true" tabindex="-1"></a><span class="co"># Remove white spaces from `dest_size`</span></span>
<span id="cb3-45"><a href="#cb3-45" aria-hidden="true" tabindex="-1"></a>airlines[<span class="st">'dest_size'</span>] <span class="op">=</span> airlines[<span class="st">'dest_size'</span>].<span class="bu">str</span>.strip()</span>
<span id="cb3-46"><a href="#cb3-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-47"><a href="#cb3-47" aria-hidden="true" tabindex="-1"></a><span class="co"># Verify changes have been effected</span></span>
<span id="cb3-48"><a href="#cb3-48" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(airlines[<span class="st">'dest_region'</span>].unique())</span>
<span id="cb3-49"><a href="#cb3-49" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(airlines[<span class="st">'dest_size'</span>].unique())</span>
<span id="cb3-50"><a href="#cb3-50" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-51"><a href="#cb3-51" aria-hidden="true" tabindex="-1"></a><span class="co"># Create ranges for categories</span></span>
<span id="cb3-52"><a href="#cb3-52" aria-hidden="true" tabindex="-1"></a>label_ranges <span class="op">=</span> [<span class="dv">0</span>, <span class="dv">60</span>, <span class="dv">180</span>, np.inf]</span>
<span id="cb3-53"><a href="#cb3-53" aria-hidden="true" tabindex="-1"></a>label_names <span class="op">=</span> [<span class="st">'short'</span>, <span class="st">'medium'</span>, <span class="st">'long'</span>]</span>
<span id="cb3-54"><a href="#cb3-54" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-55"><a href="#cb3-55" aria-hidden="true" tabindex="-1"></a><span class="co"># Create wait_type column</span></span>
<span id="cb3-56"><a href="#cb3-56" aria-hidden="true" tabindex="-1"></a>airlines[<span class="st">'wait_type'</span>] <span class="op">=</span> pd.cut(airlines[<span class="st">'wait_min'</span>], bins <span class="op">=</span> label_ranges, </span>
<span id="cb3-57"><a href="#cb3-57" aria-hidden="true" tabindex="-1"></a>                                labels <span class="op">=</span> label_names)</span>
<span id="cb3-58"><a href="#cb3-58" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-59"><a href="#cb3-59" aria-hidden="true" tabindex="-1"></a><span class="co"># Create mappings and replace</span></span>
<span id="cb3-60"><a href="#cb3-60" aria-hidden="true" tabindex="-1"></a>mappings <span class="op">=</span> {<span class="st">'Monday'</span>:<span class="st">'weekday'</span>, <span class="st">'Tuesday'</span>:<span class="st">'weekday'</span>, <span class="st">'Wednesday'</span>: <span class="st">'weekday'</span>, </span>
<span id="cb3-61"><a href="#cb3-61" aria-hidden="true" tabindex="-1"></a>            <span class="st">'Thursday'</span>: <span class="st">'weekday'</span>, <span class="st">'Friday'</span>: <span class="st">'weekday'</span>, </span>
<span id="cb3-62"><a href="#cb3-62" aria-hidden="true" tabindex="-1"></a>            <span class="st">'Saturday'</span>: <span class="st">'weekend'</span>, <span class="st">'Sunday'</span>: <span class="st">'weekend'</span>}</span>
<span id="cb3-63"><a href="#cb3-63" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-64"><a href="#cb3-64" aria-hidden="true" tabindex="-1"></a>airlines[<span class="st">'day_week'</span>] <span class="op">=</span> airlines[<span class="st">'day'</span>].replace(mappings)</span>
<span id="cb3-65"><a href="#cb3-65" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-66"><a href="#cb3-66" aria-hidden="true" tabindex="-1"></a><span class="co"># You just created two new categorical variables, that when combined with other columns, could produce really interesting analysis. Don't forget, you can always use an assert statement to check your changes passed.</span></span>
<span id="cb3-67"><a href="#cb3-67" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-68"><a href="#cb3-68" aria-hidden="true" tabindex="-1"></a><span class="co"># Replace "Dr." with empty string ""</span></span>
<span id="cb3-69"><a href="#cb3-69" aria-hidden="true" tabindex="-1"></a>airlines[<span class="st">'full_name'</span>] <span class="op">=</span> airlines[<span class="st">'full_name'</span>].<span class="bu">str</span>.replace(<span class="st">"Dr."</span>,<span class="st">""</span>)</span>
<span id="cb3-70"><a href="#cb3-70" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-71"><a href="#cb3-71" aria-hidden="true" tabindex="-1"></a><span class="co"># Replace "Mr." with empty string ""</span></span>
<span id="cb3-72"><a href="#cb3-72" aria-hidden="true" tabindex="-1"></a>airlines[<span class="st">'full_name'</span>] <span class="op">=</span> airlines[<span class="st">'full_name'</span>].<span class="bu">str</span>.replace(<span class="st">"Mr."</span>,<span class="st">""</span>)</span>
<span id="cb3-73"><a href="#cb3-73" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-74"><a href="#cb3-74" aria-hidden="true" tabindex="-1"></a><span class="co"># Replace "Miss" with empty string ""</span></span>
<span id="cb3-75"><a href="#cb3-75" aria-hidden="true" tabindex="-1"></a>airlines[<span class="st">'full_name'</span>] <span class="op">=</span> airlines[<span class="st">'full_name'</span>].<span class="bu">str</span>.replace(<span class="st">"Miss"</span>,<span class="st">""</span>)</span>
<span id="cb3-76"><a href="#cb3-76" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-77"><a href="#cb3-77" aria-hidden="true" tabindex="-1"></a><span class="co"># Replace "Ms." with empty string ""</span></span>
<span id="cb3-78"><a href="#cb3-78" aria-hidden="true" tabindex="-1"></a>airlines[<span class="st">'full_name'</span>] <span class="op">=</span> airlines[<span class="st">'full_name'</span>].<span class="bu">str</span>.replace(<span class="st">"Ms."</span>,<span class="st">""</span>)</span>
<span id="cb3-79"><a href="#cb3-79" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-80"><a href="#cb3-80" aria-hidden="true" tabindex="-1"></a><span class="co"># Assert that full_name has no honorifics</span></span>
<span id="cb3-81"><a href="#cb3-81" aria-hidden="true" tabindex="-1"></a><span class="cf">assert</span> airlines[<span class="st">'full_name'</span>].<span class="bu">str</span>.contains(<span class="st">'Ms.|Mr.|Miss|Dr.'</span>).<span class="bu">any</span>() <span class="op">==</span> <span class="va">False</span></span>
<span id="cb3-82"><a href="#cb3-82" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-83"><a href="#cb3-83" aria-hidden="true" tabindex="-1"></a><span class="co"># Store length of each row in survey_response column</span></span>
<span id="cb3-84"><a href="#cb3-84" aria-hidden="true" tabindex="-1"></a>resp_length <span class="op">=</span> airlines[<span class="st">'survey_response'</span>].<span class="bu">str</span>.<span class="bu">len</span>()</span>
<span id="cb3-85"><a href="#cb3-85" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-86"><a href="#cb3-86" aria-hidden="true" tabindex="-1"></a><span class="co"># Find rows in airlines where resp_length &gt; 40</span></span>
<span id="cb3-87"><a href="#cb3-87" aria-hidden="true" tabindex="-1"></a>airlines_survey <span class="op">=</span> airlines[resp_length <span class="op">&gt;</span> <span class="dv">40</span>]</span>
<span id="cb3-88"><a href="#cb3-88" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-89"><a href="#cb3-89" aria-hidden="true" tabindex="-1"></a><span class="co"># Assert minimum survey_response length is &gt; 40</span></span>
<span id="cb3-90"><a href="#cb3-90" aria-hidden="true" tabindex="-1"></a><span class="cf">assert</span> airlines_survey[<span class="st">'survey_response'</span>].<span class="bu">str</span>.<span class="bu">len</span>().<span class="bu">min</span>() <span class="op">&gt;</span> <span class="dv">40</span></span>
<span id="cb3-91"><a href="#cb3-91" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-92"><a href="#cb3-92" aria-hidden="true" tabindex="-1"></a><span class="co"># Print new survey_response column</span></span>
<span id="cb3-93"><a href="#cb3-93" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(airlines_survey[<span class="st">'survey_response'</span>])</span>
<span id="cb3-94"><a href="#cb3-94" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-95"><a href="#cb3-95" aria-hidden="true" tabindex="-1"></a><span class="co"># These types of feedbacks are essential to improving any service. Coupled with some wordcount analysis, you can find common patterns across all survey responses in no time!</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Cleanliness:  ['Clean' 'Average' 'Somewhat clean' 'Somewhat dirty' 'Dirty'] 

Safety:  ['Neutral' 'Very safe' 'Somewhat safe' 'Very unsafe' 'Somewhat unsafe'] 

Satisfaction:  ['Very satisfied' 'Neutral' 'Somewhat satsified' 'Somewhat unsatisfied'
 'Very unsatisfied'] 

Empty DataFrame
Columns: [id, day, airline, destination, dest_region, dest_size, boarding_area, dept_time, wait_min, cleanliness, safety, satisfaction, full_name, survey_response]
Index: []
        id        day        airline        destination    dest_region  \
0     1351    Tuesday    UNITED INTL             KANSAI           Asia   
1      373     Friday         ALASKA  SAN JOSE DEL CABO  Canada/Mexico   
2     2820   Thursday          DELTA        LOS ANGELES        West US   
3     1157    Tuesday      SOUTHWEST        LOS ANGELES        West US   
4     2992  Wednesday       AMERICAN              MIAMI        East US   
...    ...        ...            ...                ...            ...   
2804  1475    Tuesday         ALASKA       NEW YORK-JFK        East US   
2805  2222   Thursday      SOUTHWEST            PHOENIX        West US   
2806  2684     Friday         UNITED            ORLANDO        East US   
2807  2549    Tuesday        JETBLUE         LONG BEACH        West US   
2808  2162   Saturday  CHINA EASTERN            QINGDAO           Asia   

     dest_size boarding_area   dept_time  wait_min     cleanliness  \
0          Hub  Gates 91-102  12/31/2018       115           Clean   
1        Small   Gates 50-59  12/31/2018       135           Clean   
2          Hub   Gates 40-48  12/31/2018        70         Average   
3          Hub   Gates 20-39  12/31/2018       190           Clean   
4          Hub   Gates 50-59  12/31/2018       559  Somewhat clean   
...        ...           ...         ...       ...             ...   
2804       Hub   Gates 50-59  12/31/2018       280  Somewhat clean   
2805       Hub   Gates 20-39  12/31/2018       165           Clean   
2806       Hub   Gates 70-90  12/31/2018        92           Clean   
2807     Small    Gates 1-12  12/31/2018        95           Clean   
2808     Large    Gates 1-12  12/31/2018       220           Clean   

             safety        satisfaction   full_name  \
0           Neutral      Very satisfied   Mr.Stones   
1         Very safe      Very satisfied      Mr.Dan   
2     Somewhat safe             Neutral      Ms.Bob   
3         Very safe  Somewhat satsified      Dr.Mos   
4         Very safe  Somewhat satsified  Miss Volks   
...             ...                 ...         ...   
2804        Neutral  Somewhat satsified      Ms.Bob   
2805      Very safe      Very satisfied      Dr.Mos   
2806      Very safe      Very satisfied  Miss Volks   
2807  Somewhat safe      Very satisfied    Mr.Jones   
2808      Very safe  Somewhat satsified      Mr.Bim   

                       survey_response  
0                      It was terrible  
1            I did not like the flight  
2                          I hate this  
3                            Not a fan  
4                                  Bad  
...                                ...  
2804                      It was awful  
2805  My fllight was really unpleasant  
2806                    I am not a fan  
2807                I had a bad flight  
2808                   It was very bad  

[2477 rows x 14 columns]
['Asia' 'Canada/Mexico' 'West US' 'East US' 'Midwest US' 'EAST US'
 'Middle East' 'Europe' 'eur' 'Central/South America'
 'Australia/New Zealand' 'middle east']
['Hub' 'Small' '    Hub' 'Medium' 'Large' 'Hub     ' '    Small'
 'Medium     ' '    Medium' 'Small     ' '    Large' 'Large     ']
['asia' 'canada/mexico' 'west us' 'east us' 'midwest us' 'middle east'
 'europe' 'central/south america' 'australia/new zealand']
['Hub' 'Small' 'Medium' 'Large']
17      The airport personnel failed to notify us abou...
18      The food at the airport was incredibly expensi...
19      One of the other travelers was extremely loud ...
20      I don’t recall completing the survey with the ...
21      The airport personnel continuously ignored my ...
                              ...                        
2791    I was really unsatisfied with the wait times b...
2792    The flight was generally okay, but I didn’t pa...
2793    We were significantly slowed down by the secur...
2794    I felt very unsatisfied by how long the flight...
2795    There was a spill on the aisle next to the bat...
Name: survey_response, Length: 1056, dtype: object</code></pre>
</div>
</div>
</section>
</section>
<section id="chapter-3-advanced-data-problems" class="level2">
<h2 class="anchored" data-anchor-id="chapter-3-advanced-data-problems">CHAPTER 3: ADVANCED DATA PROBLEMS</h2>
</section>
<section id="chapter-3.1-uniformity" class="level2">
<h2 class="anchored" data-anchor-id="chapter-3.1-uniformity">Chapter 3.1: Uniformity</h2>
<p>Stellar work on chapter 2! You’re now an expert at handling categorical and text variables.</p>
<section id="in-this-chapter-1" class="level3">
<h3 class="anchored" data-anchor-id="in-this-chapter-1">In this chapter</h3>
<p>In this chapter, we’re looking at more advanced data cleaning problems, such as uniformity, cross field validation and dealing with missing data.</p>
</section>
<section id="data-range-constraints" class="level3">
<h3 class="anchored" data-anchor-id="data-range-constraints">Data range constraints</h3>
<p>In chapter 1, we saw how out of range values are a common problem when cleaning data, and that when left untouched, can skew your analysis.</p>
</section>
<section id="uniformity" class="level3">
<h3 class="anchored" data-anchor-id="uniformity">Uniformity</h3>
<p>In this lesson, we’re going to tackle a problem that could similarly skew our data, which is unit uniformity. For example, we can have temperature data that has values in both Fahrenheit and Celsius, weight data in Kilograms and in stones, dates in multiple formats, and so on. Verifying unit uniformity is imperative to having accurate analysis.</p>
</section>
<section id="an-example" class="level3">
<h3 class="anchored" data-anchor-id="an-example">An example</h3>
<p>Here’s a dataset with average temperature data throughout the month of March in New York City. The dataset was collected from different sources with temperature data in Celsius and Fahrenheit merged together. We can see that unless a major climate event occurred, this value here is most likely Fahrenheit, not Celsius. Let’s confirm the presence of these values visually. We can do so by plotting a scatter plot of our data. We can do this using matplotlib.pyplot, which was imported as plt. We use the plt dot scatter function, which takes in what to plot on the x axis, the y axis, and which data source to use. We set the title, axis labels with the helper functions seen here, show the plot with plt dot show,</p>
</section>
<section id="insert-title-here" class="level3">
<h3 class="anchored" data-anchor-id="insert-title-here">Insert title here…</h3>
<p>and voila. Notice these values here? They all must be fahrenheit.</p>
</section>
<section id="treating-temperature-data" class="level3">
<h3 class="anchored" data-anchor-id="treating-temperature-data">Treating temperature data</h3>
<p>A simple web search returns the formula for converting Fahrenheit to Celsius. To convert our temperature data, we isolate all rows of temperature column where it is above 40 using the loc method. We chose 40 because it’s a common sense maximum for Celsius temperatures in New York City. We then convert these values to Celsius using the formula above, and reassign them to their respective Fahrenheit values in temperatures. We can make sure that our conversion was correct with an assert statement, by making sure the maximum value of temperature is less than 40.</p>
</section>
<section id="treating-date-data" class="level3">
<h3 class="anchored" data-anchor-id="treating-date-data">Treating date data</h3>
<p>Here’s another common uniformity problem with date data. This is a DataFrame called birthdays containing birth dates for a variety of individuals. It has been collected from a variety of sources and merged into one. Notice the dates here? The one in blue has the month, day, year format, whereas the one in orange has the month written out. The one in red is obviously an error, with what looks like a day day year format. We’ll learn how to deal with that one as well.</p>
</section>
<section id="datetime-formatting" class="level3">
<h3 class="anchored" data-anchor-id="datetime-formatting">Datetime formatting</h3>
<p>We already discussed datetime objects. Without getting too much into detail, datetime accepts different formats that help you format your dates as pleased. The pandas to datetime function automatically accepts most date formats, but could raise errors when certain formats are unrecognizable. You don’t have to memorize these formats, just know that they exist and are easily searchable!</p>
</section>
<section id="treating-date-data-1" class="level3">
<h3 class="anchored" data-anchor-id="treating-date-data-1">Treating date data</h3>
<p>You can treat these date inconsistencies easily by converting your date column to datetime. We can do this in pandas with the to_datetime function. However this isn’t enough and will most likely return an error, since we have dates in multiple formats, especially the weird day/day/format which triggers an error with months. Instead we set the infer_datetime_format argument to True, and set errors equal to coerce. This will infer the format and return missing value for dates that couldn’t be identified and converted instead of a value error. This returns the birthday column with aligned formats, with the initial ambiguous format of day day year, being set to NAT, which represents missing values in Pandas for datetime objects. We can also convert the format of a datetime column using the dt dot strftime method, which accepts a datetime format of your choice. For example, here we convert the Birthday column to day month year, instead of year month day.</p>
</section>
<section id="treating-ambiguous-date-data" class="level3">
<h3 class="anchored" data-anchor-id="treating-ambiguous-date-data">Treating ambiguous date data</h3>
<p>However a common problem is having ambiguous dates with vague formats. For example, is this date value set in March or August? Unfortunately there’s no clear cut way to spot this inconsistency or to treat it. Depending on the size of the dataset and suspected ambiguities, we can either convert these dates to NAs and deal with them accordingly. If you have additional context on the source of your data, you can probably infer the format. If the majority of subsequent or previous data is of one format, you can probably infer the format as well. All in all, it is essential to properly understand where your data comes from, before trying to treat it, as it will make making these decisions much easier.</p>
</section>
</section>
<section id="exercise-3.1.1" class="level2">
<h2 class="anchored" data-anchor-id="exercise-3.1.1">Exercise 3.1.1</h2>
<section id="uniform-currencies" class="level3">
<h3 class="anchored" data-anchor-id="uniform-currencies">Uniform currencies</h3>
<p>In this exercise and throughout this chapter, you will be working with a retail banking dataset stored in the <code>banking</code> DataFrame. The dataset contains data on the amount of money stored in accounts (<code>acct_amount</code>), their currency (<code>acct_cur</code>), amount invested (<code>inv_amount</code>), account opening date (<code>account_opened</code>), and last transaction date (<code>last_transaction</code>) that were consolidated from American and European branches.</p>
<p>You are tasked with understanding the average account size and how investments vary by the size of account, however in order to produce this analysis accurately, you first need to unify the currency amount into dollars.</p>
</section>
<section id="instructions-10" class="level3">
<h3 class="anchored" data-anchor-id="instructions-10">Instructions</h3>
<ul>
<li>Find the rows of <code>acct_cur</code> in <code>banking</code> that are equal to <code>'euro'</code> and store them in the variable <code>acct_eu</code>.</li>
<li>Find all the rows of <code>acct_amount</code> in <code>banking</code> that fit the <code>acct_eu</code> condition, and convert them to USD by multiplying them with <code>1.1</code>.</li>
<li>Find all the rows of <code>acct_cur</code> in <code>banking</code> that fit the <code>acct_eu</code> condition, set them to <code>'dollar'</code>.</li>
</ul>
</section>
</section>
<section id="exercise-3.1.2" class="level2">
<h2 class="anchored" data-anchor-id="exercise-3.1.2">Exercise 3.1.2</h2>
<section id="uniform-dates" class="level3">
<h3 class="anchored" data-anchor-id="uniform-dates">Uniform dates</h3>
<p>After having unified the currencies of your different account amounts, you want to add a temporal dimension to your analysis and see how customers have been investing their money given the size of their account over each year. The <code>account_opened</code> column represents when customers opened their accounts and is a good proxy for segmenting customer activity and investment over time.</p>
<p>However, since this data was consolidated from multiple sources, you need to make sure that all dates are of the same format. You will do so by converting this column into a <code>datetime</code> object, while making sure that the format is inferred and potentially incorrect formats are set to missing.</p>
</section>
<section id="instructions-11" class="level3">
<h3 class="anchored" data-anchor-id="instructions-11">Instructions</h3>
<ul>
<li>Print the header of <code>account_opened</code> from the <code>banking</code> DataFrame and take a look at the different results.</li>
<li>Convert the <code>account_opened</code> column to <code>datetime</code>, while making sure the date format is inferred and that erroneous formats that raise error return a missing value.</li>
<li>Extract the year from the amended <code>account_opened</code> column and assign it to the <code>acct_year</code> column.</li>
<li>Print the newly created <code>acct_year</code> column.</li>
</ul>
</section>
</section>
<section id="chapter-3.2-cross-field-validation" class="level2">
<h2 class="anchored" data-anchor-id="chapter-3.2-cross-field-validation">Chapter 3.2: Cross field validation</h2>
<p>Hi and welcome to the second lesson of this chapter! In this lesson we’ll talk about cross field validation for diagnosing dirty data.</p>
<section id="motivation-1" class="level3">
<h3 class="anchored" data-anchor-id="motivation-1">Motivation</h3>
<p>Let’s take a look at the following dataset. It contains flight statistics on the total number of passengers in economy, business and first class as well as the total passengers for each flight. We know that these columns have been collected and merged from different data sources, and a common challenge when merging data from different sources is data integrity, or more broadly making sure that our data is correct.</p>
</section>
<section id="cross-field-validation" class="level3">
<h3 class="anchored" data-anchor-id="cross-field-validation">Cross field validation</h3>
<p>This is where cross field validation comes in. Cross field validation is the use of multiple fields in your dataset to sanity check the integrity of your data. For example in our flights dataset, this could be summing economy, business and first class values and making sure they are equal to the total passengers on the plane. This could be easily done in Pandas, by first subsetting on the columns to sum, then using the sum method with the axis argument set to 1 to indicate row wise summing. We then find instances where the total passengers column is equal to the sum of the classes. And find and filter out instances of inconsistent passenger amounts by subsetting on the equality we created with brackets and the tilde symbol. Here’s another example containing user IDs, birthdays and age values for a set of users. We can for example make sure that the age and birthday columns are correct by subtracting the number of years between today’s date and each birthday. We can do this by first making sure the Birthday column is converted to datetime with the pandas to datetime function. We then create an object storing today’s date using the datetime package’s date dot today function. We then calculate the difference in years between today’s date’s year, and the year of each birthday by using the dot dt dot year attribute of the user’s Birthday column. We then find instances where the calculated ages are equal to the actual age column in the users DataFrame. We then find and filter out the instances where we have inconsistencies using subsetting with brackets and the tilde symbol on the equality we created.</p>
</section>
<section id="what-to-do-when-we-catch-inconsistencies" class="level3">
<h3 class="anchored" data-anchor-id="what-to-do-when-we-catch-inconsistencies">What to do when we catch inconsistencies?</h3>
<p>So what should be the course of action in case we spot inconsistencies with cross-field validation? Just like other data cleaning problems, there is no one size fits all solution, as often the best solution requires an in depth understanding of our dataset. We can decide to either drop inconsistent data, set it to missing and impute it, or apply some rules due to domain knowledge. All these routes and assumptions can be decided upon only when you have a good understanding of where your dataset comes from and the different sources feeding into it.</p>
</section>
</section>
<section id="exercise-3.2" class="level2">
<h2 class="anchored" data-anchor-id="exercise-3.2">Exercise 3.2</h2>
<section id="hows-our-data-integrity" class="level3">
<h3 class="anchored" data-anchor-id="hows-our-data-integrity">How’s our data integrity?</h3>
<p>New data has been merged into the <code>banking</code> DataFrame that contains details on how investments in the <code>inv_amount</code> column are allocated across four different funds A, B, C and D.</p>
<p>Furthermore, the age and birth_date of customers are now stored in the <code>Age</code> and <code>birth_date</code> columns respectively.</p>
<p>You want to understand how customers of different age groups invest. However, you want to first make sure the data you’re analyzing is correct. You will do so by cross field checking values of <code>inv_amount</code> and <code>Age</code> against the amount invested in different funds and customers’ birthdays.</p>
</section>
<section id="instructions-12" class="level3">
<h3 class="anchored" data-anchor-id="instructions-12">Instructions</h3>
<ol type="1">
<li>Find the rows where the sum of all rows of the <code>fund_columns</code> in banking are equal to the <code>inv_amount</code> column. Store the values of banking with consistent <code>inv_amount</code> in <code>consistent_inv</code>, and those with inconsistent ones in <code>inconsistent_inv</code>.</li>
<li>Store today’s date into <code>today</code>, and manually calculate <code>customers'</code> ages and store them in <code>ages_manual</code>. Find all rows of banking where the <code>age</code> column is equal to <code>ages_manual</code> and then filter banking into <code>consistent_ages</code> and <code>inconsistent_ages</code>.</li>
</ol>
</section>
</section>
<section id="chapter-3.3-completeness" class="level2">
<h2 class="anchored" data-anchor-id="chapter-3.3-completeness">Chapter 3.3: Completeness</h2>
<p>Hi and welcome to the last lesson of this chapter. In this lesson, we’re going to discuss completeness and missing data.</p>
<section id="what-is-missing-data" class="level3">
<h3 class="anchored" data-anchor-id="what-is-missing-data">What is missing data?</h3>
<p>Missing data is one of the most common and most important data cleaning problems. Essentially, missing data is when no data value is stored for a variable in an observation. Missing data is most commonly represented as NA or NaN, but can take on arbitrary values like 0 or dot. Like a lot of the problems that we’ve seen thus far in the course, it’s commonly due to technical or human errors. Missing data can take many forms, so let’s take a look at an example.</p>
</section>
<section id="airquality-example" class="level3">
<h3 class="anchored" data-anchor-id="airquality-example">Airquality example</h3>
<p>Let’s take a look at the airquality dataset. It contains temperature and CO2 measurements for different dates. We can see that the CO2 value in this row is represented as NaN We can find rows with missing values by using the dot is na method, which returns True for missing values and False for complete values across all our rows and columns. We can also chain the isna method with the sum method, which returns a breakdown of missing values per column in our dataframe. We notice that the CO2 column is the only column with missing values - let’s find out why and dig further into the nature of this missingness by first visualizing our missing values.</p>
</section>
<section id="missingno" class="level3">
<h3 class="anchored" data-anchor-id="missingno">Missingno</h3>
<p>The missingno package allows to create useful visualizations of our missing data. Digging into its details is not part of the course, but you can also check out other courses on missing data in DataCamp’s course library. We visualize the missingness of the airquality DataFrame with the msno dot matrix function, and show it with pyplot’s show function from matplotlib, which returns the following image. This matrix essentially shows how missing values are distributed across a column. We see that missing CO2 values are randomly scattered throughout the column, but is that really the case? Let’s dig deeper.</p>
</section>
<section id="airquality-example-1" class="level3">
<h3 class="anchored" data-anchor-id="airquality-example-1">Airquality example</h3>
<p>We first isolate the rows of airquality with missing CO2 values in one DataFrame, and complete CO2 values in another. Then, let’s use the describe method on each of the created DataFrames. We see that for all missing values of CO2, they occur at really low temperatures, with the mean temperature at minus 39 degrees and a minimum and maximum of -49 and -30 respectively. Let’s confirm this visually with the missngno package.</p>
</section>
<section id="insert-title-here-1" class="level3">
<h3 class="anchored" data-anchor-id="insert-title-here-1">Insert title here…</h3>
<p>We first sort the DataFrame by the temperature column. Then we input the sorted dataframe to the matrix function from msno. This leaves us with this matrix. Notice how all missing values are on the top? This is because values are sorted from smallest to largest by default. This essentially confirms that CO2 measurements are lost for really low temperatures. Must be a sensor failure!</p>
</section>
<section id="missingness-types" class="level3">
<h3 class="anchored" data-anchor-id="missingness-types">Missingness types</h3>
<p>This leads us to missingness types. Without going too much into the details, there are a variety of types of missing data. It could missing completely at random, missing at random, or missing not at random. Missing completely at random data is when there missing data completely due to randomness, and there is no relationship between missing data and remaining values, such data entry errors. Despite a slightly deceiving name, Missing at random data is when there is a relationship between missing data and other observed values, such as our CO2 data being missing for low temperatures. When data is missing not at random, there is a systematic relationship between the missing data and unobserved values. For example, when it’s really hot outside, the thermometer might stop working, so we don’t have temperature measurements for days with high temperatures. However, we have no way to tell this just from looking at the data since we can’t actually see what the missing temperatures are.</p>
</section>
<section id="how-to-deal-with-missing-data" class="level3">
<h3 class="anchored" data-anchor-id="how-to-deal-with-missing-data">How to deal with missing data?</h3>
<p>There’s a variety of ways of dealing with missing data, from dropping missing data, to imputing them with statistical measures such as mean, median or mode, or imputing them with more complicated algorithmic approaches or ones that require some machine learning. Each missingness type requires a specific approach, and each type of approach has drawbacks and positives, so make sure to dig deeper in DataCamp’s course library on dealing with missing data.</p>
</section>
<section id="dealing-with-missing-data" class="level3">
<h3 class="anchored" data-anchor-id="dealing-with-missing-data">Dealing with missing data</h3>
<p>In this lesson, we’ll just explore the simple approaches to dealing with missing data. Let’s grab another look at the header of airquality.</p>
</section>
<section id="dropping-missing-values" class="level3">
<h3 class="anchored" data-anchor-id="dropping-missing-values">Dropping missing values</h3>
<p>We can drop missing values, by using the dot dropna method, alongside the subset argument which lets us pick which column’s missing values to drop.</p>
</section>
<section id="replacing-with-statistical-measures" class="level3">
<h3 class="anchored" data-anchor-id="replacing-with-statistical-measures">Replacing with statistical measures</h3>
<p>We can also replace the missing values of CO2 with the mean value of CO2, by using the fillna method, which is in this case 1.73. Fillna takes in a dictionary with columns as keys, and the imputed value as values. We can even feed custom values into fillna pertaining to our missing data if we have enough domain knowledge about our dataset.</p>
</section>
</section>
<section id="exercise-3.3.1" class="level2">
<h2 class="anchored" data-anchor-id="exercise-3.3.1">Exercise 3.3.1</h2>
<section id="missing-investors" class="level3">
<h3 class="anchored" data-anchor-id="missing-investors">Missing investors</h3>
<p>Dealing with missing data is one of the most common tasks in data science. There are a variety of types of missingness, as well as a variety of types of solutions to missing data.</p>
<p>You just received a new version of the <code>banking</code> DataFrame containing data on the amount held and invested for new and existing customers. However, there are rows with missing <code>inv_amount</code> values.</p>
<p>You know for a fact that most customers below 25 do not have investment accounts yet, and suspect it could be driving the missingness.</p>
</section>
<section id="instructions-13" class="level3">
<h3 class="anchored" data-anchor-id="instructions-13">Instructions</h3>
<ul>
<li>Print the number of missing values by column in the <code>banking</code> DataFrame.</li>
<li>Plot and show the missingness matrix of <code>banking</code> with the <code>msno.matrix()</code> function.</li>
<li>Isolate the values of <code>banking</code> missing values of <code>inv_amount</code> into <code>missing_investors</code> and with non-missing <code>inv_amount</code> values into <code>investors</code>.</li>
<li>Sort the <code>banking</code> DataFrame by the <code>age</code> column and plot the missingness matrix of <code>banking_sorted</code>.</li>
</ul>
</section>
<section id="exercise-3.3.2" class="level3">
<h3 class="anchored" data-anchor-id="exercise-3.3.2">Exercise 3.3.2</h3>
</section>
<section id="follow-the-money" class="level3">
<h3 class="anchored" data-anchor-id="follow-the-money">Follow the money</h3>
<p>In this exercise, you’re working with another version of the <code>banking</code> DataFrame that contains missing values for both the <code>cust_id</code> column and the <code>acct_amount</code> column.</p>
<p>You want to produce analysis on how many unique customers the bank has, the average amount held by customers and more. You know that rows with missing <code>cust_id</code> don’t really help you, and that on average <code>acct_amount</code> is usually 5 times the amount of <code>inv_amount</code>.</p>
<p>In this exercise, you will drop rows of <code>banking</code> with missing <code>cust_id</code>s, and impute missing values of <code>acct_amount</code> with some domain knowledge.</p>
</section>
<section id="instructions-14" class="level3">
<h3 class="anchored" data-anchor-id="instructions-14">Instructions</h3>
<ul>
<li>Use <code>.dropna()</code> to drop missing values of the <code>cust_id</code> column in <code>banking</code> and store the results in <code>banking_fullid</code>.</li>
<li>Use <code>inv_amount</code> to compute the estimated account amounts for <code>banking_fullid</code> by setting the amounts equal to <code>inv_amount * 5</code>, and assign the results to <code>acct_imp</code>.</li>
<li>Impute the missing values of <code>acct_amount</code> in <code>banking_fullid</code> with the newly created <code>acct_imp</code> using <code>.fillna()</code>.</li>
</ul>
<div id="c743616d" class="cell" data-execution_count="3">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Import the course packages</span></span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> datetime <span class="im">as</span> dt</span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> missingno <span class="im">as</span> msno</span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> fuzzywuzzy</span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> recordlinkage </span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a>banking <span class="op">=</span> pd.read_csv(<span class="st">'datasets/banking_dirty.csv'</span>, index_col <span class="op">=</span> <span class="st">'Unnamed: 0'</span>)</span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Add an column and modify rows </span></span>
<span id="cb5-13"><a href="#cb5-13" aria-hidden="true" tabindex="-1"></a>banking[<span class="st">'acct_cur'</span>] <span class="op">=</span> <span class="st">'euro'</span><span class="co"># or pd.NA for missing values</span></span>
<span id="cb5-14"><a href="#cb5-14" aria-hidden="true" tabindex="-1"></a>banking.loc[[<span class="dv">0</span>], <span class="st">'account_opened'</span>] <span class="op">=</span> <span class="st">'2018-03-05'</span></span>
<span id="cb5-15"><a href="#cb5-15" aria-hidden="true" tabindex="-1"></a>banking.loc[[<span class="dv">2</span>], <span class="st">'account_opened'</span>] <span class="op">=</span> <span class="st">'January 26, 2018'</span></span>
<span id="cb5-16"><a href="#cb5-16" aria-hidden="true" tabindex="-1"></a>banking.loc[[<span class="dv">3</span>], <span class="st">'account_opened'</span>] <span class="op">=</span> <span class="st">'21-14-17'</span></span>
<span id="cb5-17"><a href="#cb5-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-18"><a href="#cb5-18" aria-hidden="true" tabindex="-1"></a><span class="co"># Find values of acct_cur that are equal to 'euro'</span></span>
<span id="cb5-19"><a href="#cb5-19" aria-hidden="true" tabindex="-1"></a>acct_eu <span class="op">=</span> banking[<span class="st">'acct_cur'</span>] <span class="op">==</span> <span class="st">'euro'</span></span>
<span id="cb5-20"><a href="#cb5-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-21"><a href="#cb5-21" aria-hidden="true" tabindex="-1"></a><span class="co"># Convert acct_amount where it is in euro to dollars</span></span>
<span id="cb5-22"><a href="#cb5-22" aria-hidden="true" tabindex="-1"></a>banking.loc[acct_eu, <span class="st">'acct_amount'</span>] <span class="op">=</span> banking.loc[acct_eu, <span class="st">'acct_amount'</span>] <span class="op">*</span> <span class="fl">1.1</span></span>
<span id="cb5-23"><a href="#cb5-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-24"><a href="#cb5-24" aria-hidden="true" tabindex="-1"></a><span class="co"># Unify acct_cur column by changing 'euro' values to 'dollar'</span></span>
<span id="cb5-25"><a href="#cb5-25" aria-hidden="true" tabindex="-1"></a>banking.loc[acct_eu, <span class="st">'acct_cur'</span>] <span class="op">=</span> <span class="st">'dollar'</span></span>
<span id="cb5-26"><a href="#cb5-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-27"><a href="#cb5-27" aria-hidden="true" tabindex="-1"></a><span class="co"># Assert that only dollar currency remains</span></span>
<span id="cb5-28"><a href="#cb5-28" aria-hidden="true" tabindex="-1"></a><span class="cf">assert</span> banking[<span class="st">'acct_cur'</span>].unique() <span class="op">==</span> <span class="st">'dollar'</span></span>
<span id="cb5-29"><a href="#cb5-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-30"><a href="#cb5-30" aria-hidden="true" tabindex="-1"></a><span class="co"># Print the header of account_opened</span></span>
<span id="cb5-31"><a href="#cb5-31" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(banking[<span class="st">'account_opened'</span>].head())</span>
<span id="cb5-32"><a href="#cb5-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-33"><a href="#cb5-33" aria-hidden="true" tabindex="-1"></a><span class="co"># Convert account_opened to datetime</span></span>
<span id="cb5-34"><a href="#cb5-34" aria-hidden="true" tabindex="-1"></a>banking[<span class="st">'account_opened'</span>] <span class="op">=</span> pd.to_datetime(banking[<span class="st">'account_opened'</span>],</span>
<span id="cb5-35"><a href="#cb5-35" aria-hidden="true" tabindex="-1"></a>                                           <span class="co"># Infer datetime format</span></span>
<span id="cb5-36"><a href="#cb5-36" aria-hidden="true" tabindex="-1"></a>                                           infer_datetime_format <span class="op">=</span> <span class="va">True</span>,</span>
<span id="cb5-37"><a href="#cb5-37" aria-hidden="true" tabindex="-1"></a>                                           <span class="co"># Return missing value for error</span></span>
<span id="cb5-38"><a href="#cb5-38" aria-hidden="true" tabindex="-1"></a>                                           errors <span class="op">=</span> <span class="st">'coerce'</span>) </span>
<span id="cb5-39"><a href="#cb5-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-40"><a href="#cb5-40" aria-hidden="true" tabindex="-1"></a><span class="co"># Get year of account opened</span></span>
<span id="cb5-41"><a href="#cb5-41" aria-hidden="true" tabindex="-1"></a>banking[<span class="st">'acct_year'</span>] <span class="op">=</span> banking[<span class="st">'account_opened'</span>].dt.strftime(<span class="st">'%Y'</span>)</span>
<span id="cb5-42"><a href="#cb5-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-43"><a href="#cb5-43" aria-hidden="true" tabindex="-1"></a><span class="co"># Print acct_year</span></span>
<span id="cb5-44"><a href="#cb5-44" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(banking[<span class="st">'acct_year'</span>])</span>
<span id="cb5-45"><a href="#cb5-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-46"><a href="#cb5-46" aria-hidden="true" tabindex="-1"></a>banking[<span class="st">'birth_date'</span>].info()</span>
<span id="cb5-47"><a href="#cb5-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-48"><a href="#cb5-48" aria-hidden="true" tabindex="-1"></a><span class="co"># Store fund columns to sum against</span></span>
<span id="cb5-49"><a href="#cb5-49" aria-hidden="true" tabindex="-1"></a>fund_columns <span class="op">=</span> [<span class="st">'fund_A'</span>, <span class="st">'fund_B'</span>, <span class="st">'fund_C'</span>, <span class="st">'fund_D'</span>]</span>
<span id="cb5-50"><a href="#cb5-50" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-51"><a href="#cb5-51" aria-hidden="true" tabindex="-1"></a><span class="co"># Find rows where fund_columns row sum == inv_amount</span></span>
<span id="cb5-52"><a href="#cb5-52" aria-hidden="true" tabindex="-1"></a>inv_equ <span class="op">=</span> banking[fund_columns].<span class="bu">sum</span>(axis<span class="op">=</span><span class="dv">1</span>) <span class="op">==</span> banking[<span class="st">'inv_amount'</span>]</span>
<span id="cb5-53"><a href="#cb5-53" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-54"><a href="#cb5-54" aria-hidden="true" tabindex="-1"></a><span class="co"># Store consistent and inconsistent data</span></span>
<span id="cb5-55"><a href="#cb5-55" aria-hidden="true" tabindex="-1"></a>consistent_inv <span class="op">=</span> banking[inv_equ]</span>
<span id="cb5-56"><a href="#cb5-56" aria-hidden="true" tabindex="-1"></a>inconsistent_inv <span class="op">=</span> banking[<span class="op">~</span>inv_equ]</span>
<span id="cb5-57"><a href="#cb5-57" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-58"><a href="#cb5-58" aria-hidden="true" tabindex="-1"></a><span class="co"># Store consistent and inconsistent data</span></span>
<span id="cb5-59"><a href="#cb5-59" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Number of inconsistent investments: "</span>, inconsistent_inv.shape[<span class="dv">0</span>])</span>
<span id="cb5-60"><a href="#cb5-60" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Number of consistent investments: "</span>, consistent_inv.shape[<span class="dv">0</span>])</span>
<span id="cb5-61"><a href="#cb5-61" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-62"><a href="#cb5-62" aria-hidden="true" tabindex="-1"></a><span class="co"># Convert birth_date to datetime</span></span>
<span id="cb5-63"><a href="#cb5-63" aria-hidden="true" tabindex="-1"></a>banking[<span class="st">'birth_date'</span>] <span class="op">=</span> pd.to_datetime(banking[<span class="st">'birth_date'</span>],</span>
<span id="cb5-64"><a href="#cb5-64" aria-hidden="true" tabindex="-1"></a>                                           <span class="co"># Infer datetime format</span></span>
<span id="cb5-65"><a href="#cb5-65" aria-hidden="true" tabindex="-1"></a>                                           infer_datetime_format <span class="op">=</span> <span class="va">True</span>,</span>
<span id="cb5-66"><a href="#cb5-66" aria-hidden="true" tabindex="-1"></a>                                           <span class="co"># Return missing value for error</span></span>
<span id="cb5-67"><a href="#cb5-67" aria-hidden="true" tabindex="-1"></a>                                           errors <span class="op">=</span> <span class="st">'coerce'</span>) </span>
<span id="cb5-68"><a href="#cb5-68" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-69"><a href="#cb5-69" aria-hidden="true" tabindex="-1"></a><span class="co"># Store today's date and find ages</span></span>
<span id="cb5-70"><a href="#cb5-70" aria-hidden="true" tabindex="-1"></a>today <span class="op">=</span> dt.date.today()</span>
<span id="cb5-71"><a href="#cb5-71" aria-hidden="true" tabindex="-1"></a>ages_manual <span class="op">=</span> today.year <span class="op">-</span> banking[<span class="st">'birth_date'</span>].dt.year</span>
<span id="cb5-72"><a href="#cb5-72" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-73"><a href="#cb5-73" aria-hidden="true" tabindex="-1"></a><span class="co"># Find rows where age column == ages_manual</span></span>
<span id="cb5-74"><a href="#cb5-74" aria-hidden="true" tabindex="-1"></a>age_equ <span class="op">=</span> ages_manual <span class="op">==</span> banking[<span class="st">'Age'</span>]</span>
<span id="cb5-75"><a href="#cb5-75" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-76"><a href="#cb5-76" aria-hidden="true" tabindex="-1"></a><span class="co"># Store consistent and inconsistent data</span></span>
<span id="cb5-77"><a href="#cb5-77" aria-hidden="true" tabindex="-1"></a>consistent_ages <span class="op">=</span> banking[age_equ]</span>
<span id="cb5-78"><a href="#cb5-78" aria-hidden="true" tabindex="-1"></a>inconsistent_ages <span class="op">=</span> banking[<span class="op">~</span>age_equ]</span>
<span id="cb5-79"><a href="#cb5-79" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-80"><a href="#cb5-80" aria-hidden="true" tabindex="-1"></a><span class="co"># Store consistent and inconsistent data</span></span>
<span id="cb5-81"><a href="#cb5-81" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Number of inconsistent ages: "</span>, inconsistent_ages.shape[<span class="dv">0</span>])</span>
<span id="cb5-82"><a href="#cb5-82" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Number of consistent ages: "</span>, consistent_ages.shape[<span class="dv">0</span>])</span>
<span id="cb5-83"><a href="#cb5-83" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-84"><a href="#cb5-84" aria-hidden="true" tabindex="-1"></a><span class="co"># Print number of missing values in banking</span></span>
<span id="cb5-85"><a href="#cb5-85" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(banking.isna().<span class="bu">sum</span>())</span>
<span id="cb5-86"><a href="#cb5-86" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-87"><a href="#cb5-87" aria-hidden="true" tabindex="-1"></a><span class="co"># Visualize missingness matrix</span></span>
<span id="cb5-88"><a href="#cb5-88" aria-hidden="true" tabindex="-1"></a>msno.matrix(banking)</span>
<span id="cb5-89"><a href="#cb5-89" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb5-90"><a href="#cb5-90" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-91"><a href="#cb5-91" aria-hidden="true" tabindex="-1"></a><span class="co"># Isolate missing and non missing values of inv_amount</span></span>
<span id="cb5-92"><a href="#cb5-92" aria-hidden="true" tabindex="-1"></a>missing_investors <span class="op">=</span> banking[banking[<span class="st">'inv_amount'</span>].isna()]</span>
<span id="cb5-93"><a href="#cb5-93" aria-hidden="true" tabindex="-1"></a>investors <span class="op">=</span> banking[<span class="op">~</span>banking[<span class="st">'inv_amount'</span>].isna()]</span>
<span id="cb5-94"><a href="#cb5-94" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-95"><a href="#cb5-95" aria-hidden="true" tabindex="-1"></a><span class="co"># Describe complete DataFrame</span></span>
<span id="cb5-96"><a href="#cb5-96" aria-hidden="true" tabindex="-1"></a>investors.describe()</span>
<span id="cb5-97"><a href="#cb5-97" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-98"><a href="#cb5-98" aria-hidden="true" tabindex="-1"></a><span class="co"># Describe missing DataFrame</span></span>
<span id="cb5-99"><a href="#cb5-99" aria-hidden="true" tabindex="-1"></a>missing_investors.describe()</span>
<span id="cb5-100"><a href="#cb5-100" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-101"><a href="#cb5-101" aria-hidden="true" tabindex="-1"></a><span class="co"># Sort banking by age and visualize</span></span>
<span id="cb5-102"><a href="#cb5-102" aria-hidden="true" tabindex="-1"></a>banking_sorted <span class="op">=</span> banking.sort_values(by<span class="op">=</span><span class="st">'Age'</span>)</span>
<span id="cb5-103"><a href="#cb5-103" aria-hidden="true" tabindex="-1"></a>msno.matrix(banking_sorted)</span>
<span id="cb5-104"><a href="#cb5-104" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb5-105"><a href="#cb5-105" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-106"><a href="#cb5-106" aria-hidden="true" tabindex="-1"></a><span class="co"># Drop missing values of cust_id</span></span>
<span id="cb5-107"><a href="#cb5-107" aria-hidden="true" tabindex="-1"></a>banking_fullid <span class="op">=</span> banking.dropna(subset <span class="op">=</span> [<span class="st">'cust_id'</span>])</span>
<span id="cb5-108"><a href="#cb5-108" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-109"><a href="#cb5-109" aria-hidden="true" tabindex="-1"></a><span class="co"># Compute estimated acct_amount</span></span>
<span id="cb5-110"><a href="#cb5-110" aria-hidden="true" tabindex="-1"></a>acct_imp <span class="op">=</span> banking_fullid[<span class="st">'inv_amount'</span>] <span class="op">*</span> <span class="dv">5</span></span>
<span id="cb5-111"><a href="#cb5-111" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-112"><a href="#cb5-112" aria-hidden="true" tabindex="-1"></a><span class="co"># Impute missing acct_amount with corresponding acct_imp</span></span>
<span id="cb5-113"><a href="#cb5-113" aria-hidden="true" tabindex="-1"></a>banking_imputed <span class="op">=</span> banking_fullid.fillna({<span class="st">'acct_amount'</span>: acct_imp})</span>
<span id="cb5-114"><a href="#cb5-114" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-115"><a href="#cb5-115" aria-hidden="true" tabindex="-1"></a><span class="co"># Print number of missing values</span></span>
<span id="cb5-116"><a href="#cb5-116" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(banking_imputed.isna().<span class="bu">sum</span>())</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>0          2018-03-05
1            28-02-19
2    January 26, 2018
3            21-14-17
4            14-05-18
Name: account_opened, dtype: object
0     2018
1      NaN
2      NaN
3      NaN
4      NaN
      ... 
95     NaN
96     NaN
97     NaN
98     NaN
99     NaN
Name: acct_year, Length: 100, dtype: object
&lt;class 'pandas.core.series.Series'&gt;
Index: 100 entries, 0 to 99
Series name: birth_date
Non-Null Count  Dtype 
--------------  ----- 
100 non-null    object
dtypes: object(1)
memory usage: 5.6+ KB
Number of inconsistent investments:  8
Number of consistent investments:  92
Number of inconsistent ages:  92
Number of consistent ages:  8
cust_id              0
birth_date           0
Age                  0
acct_amount          0
inv_amount           0
fund_A               0
fund_B               0
fund_C               0
fund_D               0
account_opened      99
last_transaction     0
acct_cur             0
acct_year           99
dtype: int64</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="index_files/figure-html/cell-4-output-2.png" width="1963" height="919" class="figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="index_files/figure-html/cell-4-output-3.png" width="1963" height="919" class="figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>cust_id              0
birth_date           0
Age                  0
acct_amount          0
inv_amount           0
fund_A               0
fund_B               0
fund_C               0
fund_D               0
account_opened      99
last_transaction     0
acct_cur             0
acct_year           99
dtype: int64</code></pre>
</div>
</div>
</section>
</section>
<section id="there-are-only-8-and-92-rows-affected-by-inconsistent-inv_amount-and-age-values-respectively.-in-this-case-its-best-to-investigate-the-underlying-data-sources-before-deciding-on-a-course-of-action" class="level1">
<h1>There are only 8 and 92 rows affected by <em>inconsistent inv_amount and age values</em>, respectively. In this case, it’s best to investigate the underlying data sources before deciding on a course of action!</h1>
<section id="chapter-4-record-linkage" class="level2">
<h2 class="anchored" data-anchor-id="chapter-4-record-linkage">CHAPTER 4: RECORD LINKAGE</h2>
</section>
<section id="chapter-4.1-comparing-strings" class="level2">
<h2 class="anchored" data-anchor-id="chapter-4.1-comparing-strings">Chapter 4.1: Comparing strings</h2>
<p>Awesome work on chapter 3! Welcome to the final chapter of this course, where we’ll discover the world of record linkage. But before we get deep dive into record linkage, let’s sharpen our understanding of string similarity and minimum edit distance. Minimum edit distance Minimum edit distance is a systematic way to identify how close 2 strings are. For example, let’s take a look at the following two words: intention, and execution. The minimum edit distance between them is the least possible amount of steps, that could get us from the word intention to execution, with the available operations being inserting new characters, deleting them, substituting them, and transposing consecutive characters. To get from intention to execution, we first start off by deleting I from intention, and adding C between E and N. Our minimum edit distance so far is 2, since these are two operations. Then we substitute the first N with E, T with X, and N with U, leading us to execution! With the minimum edit distance being 5. The lower the edit distance, the closer two words are. For example, the two different typos of reading have a minimum edit distance of 1 between them and reading.</p>
<section id="minimum-edit-distance-algorithms" class="level3">
<h3 class="anchored" data-anchor-id="minimum-edit-distance-algorithms">Minimum edit distance algorithms</h3>
<p>There’s a variety of algorithms based on edit distance that differ on which operations they use, how much weight attributed to each operation, which type of strings they’re suited for and more, with a variety of packages to get each similarity. For this lesson, we’ll be comparing strings using Levenshtein distance since it’s the most general form of string matching by using the thefuzz package.</p>
</section>
<section id="simple-string-comparison" class="level3">
<h3 class="anchored" data-anchor-id="simple-string-comparison">Simple string comparison</h3>
<p>thefuzz is a package to perform string comparison. We first import fuzz from thefuzz, which allow us to compare between single strings. Here we use fuzz’s WRatio function to compute the similarity between reading and its typo, inputting each string as an argument. For any comparison function using thefuzz, our output is a score from 0 to 100 with 0 being not similar at all, 100 being an exact match. Do not confuse this with the minimum edit distance score from earlier, where a lower minimum edit distance means a closer match.</p>
</section>
<section id="partial-strings-and-different-orderings" class="level3">
<h3 class="anchored" data-anchor-id="partial-strings-and-different-orderings">Partial strings and different orderings</h3>
<p>The WRatio function is highly robust against partial string comparison with different orderings. For example here we compare the strings Houston Rockets and Rockets, and still receive a high similarity score. The same can be said for the strings Houston Rockets vs Los Angeles Lakers and Lakers vs Rockets, where the team names are only partial and they are differently ordered.</p>
</section>
<section id="comparison-with-arrays" class="level3">
<h3 class="anchored" data-anchor-id="comparison-with-arrays">Comparison with arrays</h3>
<p>We can also compare a string with an array of strings by using the extract function from the process module from fuzzy wuzzy. Extract takes in a string, an array of strings, and the number of possible matches to return ranked from highest to lowest. It returns a list of tuples with 3 elements, the first one being the matching string being returned, the second one being its similarity score, and the third one being its index in the array.</p>
</section>
<section id="collapsing-categories-with-string-similarity" class="level3">
<h3 class="anchored" data-anchor-id="collapsing-categories-with-string-similarity">Collapsing categories with string similarity</h3>
<p>In chapter 2, we learned that collapsing data into categories is an essential aspect of working with categorical and text data, and we saw how to manually replace categories in a column of a DataFrame. But what if we had so many inconsistent categories that a manual replacement is simply not feasible? We can easily do that with string similarity!</p>
</section>
<section id="collapsing-categories-with-string-matching" class="level3">
<h3 class="anchored" data-anchor-id="collapsing-categories-with-string-matching">Collapsing categories with string matching</h3>
<p>Say we have DataFrame named survey containing answers from respondents from the state of New York and California asking them how likely are you to move on a scale of 0 to 5. The state field was free text and contains hundreds of typos. Remapping them manually would take a huge amount of time. Instead, we’ll use string similarity. We also have a category DataFrame containing the correct categories for each state. Let’s collapse the incorrect categories with string matching!</p>
</section>
<section id="collapsing-all-of-the-state" class="level3">
<h3 class="anchored" data-anchor-id="collapsing-all-of-the-state">Collapsing all of the state</h3>
<p>We first create a for loop iterating over each correctly typed state in the categories DataFrame. For each state, we find its matches in the state column of the survey DataFrame, returning all possible matches by setting the limit argument of extract to the length of the survey DataFrame. Then we iterate over each potential match, isolating the ones only with a similarity score higher or equal than 80 with an if statement. Then for each of those returned strings, we replace it with the correct state using the loc method.</p>
</section>
<section id="record-linkage" class="level3">
<h3 class="anchored" data-anchor-id="record-linkage">Record linkage</h3>
<p>Record linkage attempts to join data sources that have similarly fuzzy duplicate values, so that we end up with a final DataFrame with no duplicates by using string similarity. We’ll cover record linkage in more detail in the next couple of lessons.</p>
</section>
</section>
<section id="exercise-4.1.1" class="level2">
<h2 class="anchored" data-anchor-id="exercise-4.1.1">Exercise 4.1.1</h2>
<section id="the-cutoff-point" class="level3">
<h3 class="anchored" data-anchor-id="the-cutoff-point">The cutoff point</h3>
<p>In this exercise, and throughout this chapter, you’ll be working with the <code>restaurants</code> DataFrame which has data on various restaurants. Your ultimate goal is to create a restaurant recommendation engine, but you need to first clean your data.</p>
<p>This version of <code>restaurants</code> has been collected from many sources, where the <code>cuisine_type</code> column is riddled with typos, and should contain only <code>italian</code>, <code>american</code> and <code>asian</code> cuisine types. There are so many unique categories that remapping them manually isn’t scalable, and it’s best to use string similarity instead.</p>
<p>Before doing so, you want to establish the cutoff point for the similarity score using the <code>thefuzz</code>’s <code>process.extract()</code> function by finding the similarity score of the most distant typo of each category.</p>
</section>
<section id="instructions-15" class="level3">
<h3 class="anchored" data-anchor-id="instructions-15">Instructions</h3>
<ul>
<li>Import <code>process</code> from <code>thefuzz</code>.</li>
<li>Store the unique <code>cuisine_types</code> into <code>unique_types</code>.</li>
<li>Calculate the similarity of <code>'asian'</code>, <code>'american'</code>, and <code>'italian'</code> to all possible <code>cuisine_types</code> using <code>process.extract()</code>, while returning all possible matches.</li>
</ul>
</section>
<section id="remapping-categories-ii" class="level3">
<h3 class="anchored" data-anchor-id="remapping-categories-ii">Remapping categories II</h3>
<p>In the last exercise, you determined that the distance cutoff point for remapping typos of <code>'american'</code>, <code>'asian'</code>, and <code>'italian'</code> cuisine types stored in the <code>cuisine_type</code> column should be 80.</p>
<p>In this exercise, you’re going to put it all together by finding matches with similarity scores equal to or higher than 80 by using <code>fuzywuzzy.process</code>’s <code>extract()</code> function, for each correct cuisine type, and replacing these matches with it. Remember, when comparing a string with an array of strings using <code>process.extract()</code>, the output is a list of tuples where each is formatted like:</p>
<pre><code>(closest match, similarity score, index of match)</code></pre>
</section>
<section id="instructions-16" class="level3">
<h3 class="anchored" data-anchor-id="instructions-16">Instructions</h3>
<ul>
<li>Return all of the unique values in the <code>cuisine_type</code> column of <code>restaurants</code>.</li>
<li>As a first step, create a list of all possible matches, comparing <code>'italian'</code> with the restaurant types listed in the <code>cuisine_type</code> column.</li>
<li>Within the <code>for loop</code>, use an <code>if statement</code> to check whether the similarity score in each match is greater than or equal to 80.</li>
<li>If it is, use <code>.loc</code> to select rows where <code>cuisine_type</code> in restaurants is equal to the current match (which is the first element of match), and reassign them to be <code>'italian'</code>.</li>
<li>Using the variable <code>cuisine</code> to iterate through <code>categories</code>, embed your code from the previous step in an outer <code>for loop</code>.</li>
<li>Inspect the final result.</li>
</ul>
</section>
</section>
<section id="chapter-4.2-generating-pairs" class="level2">
<h2 class="anchored" data-anchor-id="chapter-4.2-generating-pairs">Chapter 4.2: Generating pairs</h2>
<p>Great work with lesson 1 - you now have a solid understanding how to calculate string similarity.</p>
<section id="motivation-2" class="level3">
<h3 class="anchored" data-anchor-id="motivation-2">Motivation</h3>
<p>At the end of the last video exercise, we saw how record linkage attempts to join data sources with fuzzy duplicate values. For example here are two DataFrames containing NBA games and their schedules. They’ve both been scraped from different sites and we would want to merge them together and have one DataFrame containing all unique games.</p>
</section>
<section id="when-joins-wont-work" class="level3">
<h3 class="anchored" data-anchor-id="when-joins-wont-work">When joins won’t work</h3>
<p>We see that there are duplicates values in both DataFrames with different naming marked here in red, and non duplicate values, marked here in green. Since there are games happening at the same time, no common unique identifier between the DataFrames, and the events are differently named, a regular join or merge will not work. This is where record linkage comes in.</p>
</section>
<section id="record-linkage-1" class="level3">
<h3 class="anchored" data-anchor-id="record-linkage-1">Record linkage</h3>
<p>Record linkage is the act of linking data from different sources regarding the same entity. Generally, we clean two or more DataFrames, generate pairs of potentially matching records, score these pairs according to string similarity and other similarity metrics, and link them. All of these steps can be achieved with the recordlinkage package, let’s find how!</p>
</section>
<section id="our-dataframes" class="level3">
<h3 class="anchored" data-anchor-id="our-dataframes">Our DataFrames</h3>
<p>Here we have two DataFrames, census_A, and census_B, containing data on individuals throughout the states. We want to merge them while avoiding duplication using record linkage, since they are collected manually and are prone to typos, there are no consistent IDs between them.</p>
</section>
<section id="generating-pairs" class="level3">
<h3 class="anchored" data-anchor-id="generating-pairs">Generating pairs</h3>
<p>We first want to generate pairs between both DataFrames. Ideally, we want to generate all possible pairs between our DataFrames. But what if we had big DataFrames and ended up having to generate millions if not billions of pairs? It wouldn’t prove scalable and could seriously hamper development time.</p>
</section>
<section id="blocking" class="level3">
<h3 class="anchored" data-anchor-id="blocking">Blocking</h3>
<p>This is where we apply what we call blocking, which creates pairs based on a matching column, which is in this case, the state column, reducing the number of possible pairs.</p>
</section>
<section id="generating-pairs-1" class="level3">
<h3 class="anchored" data-anchor-id="generating-pairs-1">Generating pairs</h3>
<p>To do this, we first start off by importing recordlinkage. We then use the recordlinkage dot Index function, to create an indexing object. This essentially is an object we can use to generate pairs from our DataFrames. To generate pairs blocked on state, we use the block method, inputting the state column as input. Once the indexer object has been initialized, we generate our pairs using the dot index method, which takes in the two dataframes. The resulting object, is a pandas multi index object containing pairs of row indices from both DataFrames, which is a fancy way to say it is an array containing possible pairs of indices that makes it much easier to subset DataFrames on.</p>
</section>
<section id="comparing-the-dataframes" class="level3">
<h3 class="anchored" data-anchor-id="comparing-the-dataframes">Comparing the DataFrames</h3>
<p>Since we’ve already generated our pairs, it’s time to find potential matches. We first start by creating a comparison object using the recordlinkage dot compare function. This is similar to the indexing object we created while generating pairs, but this one is responsible for assigning different comparison procedures for pairs. Let’s say there are columns for which we want exact matches between the pairs. To do that, we use the exact method. It takes in the column name in question for each DataFrame, which is in this case date_of_birth and state, and a label argument which lets us set the column name in the resulting DataFrame. Now in order to compute string similarities between pairs of rows for columns that have fuzzy values, we use the dot string method, which also takes in the column names in question, the similarity cutoff point in the threshold argument, which takes in a value between 0 and 1, which we here set to 0.85. Finally to compute the matches, we use the compute function, which takes in the possible pairs, and the two DataFrames in question. Note that you need to always have the same order of DataFrames when inserting them as arguments when generating pairs, comparing between columns, and computing comparisons.</p>
</section>
<section id="finding-matching-pairs" class="level3">
<h3 class="anchored" data-anchor-id="finding-matching-pairs">Finding matching pairs</h3>
<p>The output is a multi index DataFrame, where the first index is the row index from the first DataFrame, or census A, and the second index is a list of all row indices in census B. The columns are the columns being compared, with values being 1 for a match, and 0 for not a match.</p>
</section>
<section id="finding-the-only-pairs-we-want" class="level3">
<h3 class="anchored" data-anchor-id="finding-the-only-pairs-we-want">Finding the only pairs we want</h3>
<p>To find potential matches, we just filter for rows where the sum of row values is higher than a certain threshold. Which in this case higher or equal to 2. But we’ll dig deeper into these matches and see how to use them to link our census DataFrames in the next lesson.</p>
</section>
</section>
<section id="exercise-4.2.1" class="level2">
<h2 class="anchored" data-anchor-id="exercise-4.2.1">Exercise 4.2.1</h2>
<section id="pairs-of-restaurants" class="level3">
<h3 class="anchored" data-anchor-id="pairs-of-restaurants">Pairs of restaurants</h3>
<p>In the last lesson, you cleaned the <code>restaurants</code> dataset to make it ready for building a restaurants recommendation engine. You have a new DataFrame named <code>restaurants_new</code> with new restaurants to train your model on, that’s been scraped from a new data source.</p>
<p>You’ve already cleaned the <code>cuisine_type</code> and <code>city</code> columns using the techniques learned throughout the course. However you saw duplicates with typos in <code>restaurants</code> names that require record linkage instead of joins with <code>restaurants</code>.</p>
<p>In this exercise, you will perform the first step in record linkage and generate possible pairs of rows between <code>restaurants</code> and <code>restaurants_new</code>.</p>
</section>
<section id="instructions-17" class="level3">
<h3 class="anchored" data-anchor-id="instructions-17">Instructions</h3>
<ul>
<li>Instantiate an indexing object by using the <code>Index()</code> function from <code>recordlinkage</code>.</li>
<li>Block your pairing on <code>cuisine_type</code> by using <code>indexer</code>‘s’ <code>.block()</code> method.</li>
<li>Generate pairs by indexing <code>restaurants</code> and <code>restaurants_new</code> in that order.</li>
</ul>
</section>
</section>
<section id="exercise-4.2.2" class="level2">
<h2 class="anchored" data-anchor-id="exercise-4.2.2">Exercise 4.2.2</h2>
<section id="similar-restaurants" class="level3">
<h3 class="anchored" data-anchor-id="similar-restaurants">Similar restaurants</h3>
<p>In the last exercise, you generated pairs between <code>restaurants</code> and <code>restaurants_new</code> in an effort to cleanly merge both DataFrames using record linkage.</p>
<p>When performing record linkage, there are different types of matching you can perform between different columns of your DataFrames, including exact matches, string similarities, and more.</p>
<p>Now that your pairs have been generated and stored in pairs, you will find exact matches in the <code>city</code> and <code>cuisine_type</code> columns between each pair, and similar strings for each pair in the <code>name</code> column.</p>
</section>
<section id="instructions-18" class="level3">
<h3 class="anchored" data-anchor-id="instructions-18">Instructions</h3>
<ul>
<li>Instantiate a comparison object using the <code>recordlinkage.Compare()</code> function.</li>
<li>Use the appropriate <code>comp_cl</code> method to find exact matches between the <code>city</code> and <code>cuisine_type</code> columns of both DataFrames.</li>
<li>Use the appropriate <code>comp_cl</code> method to find similar strings with a <code>0.8</code> similarity threshold in the <code>name</code> column of both DataFrames.</li>
<li>Compute the comparison of the pairs by using the <code>.compute()</code> method of <code>comp_cl</code>.</li>
<li>Print out <code>potential_matches</code>, the columns are the columns being compared, with values being 1 for a match, and 0 for not a match for each pair of rows in your DataFrames. To find potential matches, you need to find rows with more than matching value in a column. You can find them with</li>
</ul>
<pre><code>potential_matches[potential_matches.sum(axis = 1) &gt;= n]</code></pre>
<p>Where <code>n</code> is the minimum number of columns you want matching to ensure a proper duplicate find, what do you think should the value of <code>n</code> be?</p>
</section>
</section>
<section id="chapter-4.3-linking-dataframes" class="level2">
<h2 class="anchored" data-anchor-id="chapter-4.3-linking-dataframes">Chapter 4.3: Linking DataFrames</h2>
<p>Awesome work on the first 2 lessons! You’ve made it to the last lesson of this course!</p>
<section id="record-linkage-2" class="level3">
<h3 class="anchored" data-anchor-id="record-linkage-2">Record linkage</h3>
<p>At this point, you’ve generated your pairs, compared them, and scored them. Now it’s time to link your data!</p>
</section>
<section id="our-dataframes-1" class="level3">
<h3 class="anchored" data-anchor-id="our-dataframes-1">Our DataFrames</h3>
<p>Remember our census DataFrames from the video of the previous lesson?</p>
</section>
<section id="what-weve-already-done" class="level3">
<h3 class="anchored" data-anchor-id="what-weve-already-done">What we’ve already done</h3>
<p>We’ve already generated pairs between them, compared four of their columns, two for exact matches and two for string similarity alongside a 0.85 threshold, and found potential matches.</p>
</section>
<section id="what-were-doing-now" class="level3">
<h3 class="anchored" data-anchor-id="what-were-doing-now">What we’re doing now</h3>
<p>Now it’s time to link both census DataFrames.</p>
</section>
<section id="our-potential-matches" class="level3">
<h3 class="anchored" data-anchor-id="our-potential-matches">Our potential matches</h3>
<p>Let’s look closely at our potential matches. It is a multi-index DataFrame, where we have two index columns, record id 1, and record id 2. The first index column, stores indices from census A. The second index column, stores all possible indices from census_B, for each row index of census_A. The columns of our potential matches are the columns we chose to link both DataFrames on, where the value is 1 for a match, and 0 otherwise.</p>
</section>
<section id="probable-matches" class="level3">
<h3 class="anchored" data-anchor-id="probable-matches">Probable matches</h3>
<p>The first step in linking DataFrames, is to isolate the potentially matching pairs to the ones we’re pretty sure of. We saw how to do this in the previous lesson, by subsetting the rows where the row sum is above a certain number of columns, in this case 3. The output is row indices between census A and census B that are most likely duplicates. Our next step is to extract the one of the index columns, and subsetting its associated DataFrame to filter for duplicates. Here we choose the second index column, which represents row indices of census B. We want to extract those indices, and subset census_B on them to remove duplicates with census_A before appending them together.</p>
</section>
<section id="get-the-indices" class="level3">
<h3 class="anchored" data-anchor-id="get-the-indices">Get the indices</h3>
<p>We can access a DataFrame’s index using the index attribute. Since this is a multi index DataFrame, it returns a multi index object containing pairs of row indices from census_A and census_B respectively. We want to extract all census_B indices, so we chain it with the get_level_values method, which takes in which column index we want to extract its values. We can either input the index column’s name, or its order, which is in this case 1. To find the duplicates in census B, we simply subset on all indices of census_B, with the ones found through record linkage. You can choose to examine them further for similarity with their duplicates in census_A, but if you’re sure of your analysis, you can go ahead and find the non duplicates by repeating the exact same line of code, except by adding a tilde at the beginning of your subset. Now that you have your non duplicates, all you need is a simple append using the DataFrame append method of census A, and you have your linked Data! To recap, what we did was build on top of our previous work in generating pairs, comparing across columns and finding potential matches. We then isolated all possible matches, where there are matches across 3 columns or more, ensuring we tightened our search for duplicates across both DataFrames before we link them. Extracted the row indices of census_B where there are duplicates. Found rows of census_B where they are not duplicated with census_A by using the tilde symbol. And linked both DataFrames for full census results!</p>
</section>
</section>
<section id="exercise-4.3" class="level2">
<h2 class="anchored" data-anchor-id="exercise-4.3">Exercise 4.3</h2>
<section id="linking-them-together" class="level3">
<h3 class="anchored" data-anchor-id="linking-them-together">Linking them together!</h3>
<p>In the last lesson, you’ve finished the bulk of the work on your effort to link <code>restaurants</code> and <code>restaurants_new</code>. You’ve generated the different pairs of potentially matching rows, searched for exact matches between the <code>cuisine_type</code> and <code>city</code> columns, but compared for similar strings in the <code>name</code> column. You stored the DataFrame containing the scores in <code>potential_matches</code>.</p>
<p>Now it’s finally time to link both DataFrames. You will do so by first extracting all row indices of <code>restaurants_new</code> that are matching across the columns mentioned above from <code>potential_matches</code>. Then you will subset <code>restaurants_new</code> on these indices, then append the non-duplicate values to <code>restaurants</code>.</p>
</section>
<section id="instructions-19" class="level3">
<h3 class="anchored" data-anchor-id="instructions-19">Instructions</h3>
<ul>
<li>Isolate instances of <code>potential_matches</code> where the row sum is above or equal to 3 by using the <code>.sum()</code> method.</li>
<li>Extract the second column index from <code>matches</code>, which represents row indices of matching record from <code>restaurants_new</code> by using the <code>.get_level_values()</code> method.</li>
<li>Subset <code>restaurants_new</code> for rows that are not in <code>matching_indices</code>.</li>
<li>Append <code>non_dup</code> to <code>restaurants</code>.</li>
</ul>
<div id="656f65d9" class="cell" data-execution_count="4">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Import the course packages</span></span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> datetime <span class="im">as</span> dt</span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> missingno <span class="im">as</span> msno</span>
<span id="cb10-7"><a href="#cb10-7" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> fuzzywuzzy</span>
<span id="cb10-8"><a href="#cb10-8" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> recordlinkage </span>
<span id="cb10-9"><a href="#cb10-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-10"><a href="#cb10-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Import the course datasets</span></span>
<span id="cb10-11"><a href="#cb10-11" aria-hidden="true" tabindex="-1"></a>restaurants <span class="op">=</span> pd.read_csv(<span class="st">'datasets/restaurants.mod.csv'</span>, index_col <span class="op">=</span> <span class="st">'Unnamed: 0'</span>)</span>
<span id="cb10-12"><a href="#cb10-12" aria-hidden="true" tabindex="-1"></a>restaurants_new <span class="op">=</span> pd.read_csv(<span class="st">'datasets/restaurants_new.mod.csv'</span>, index_col <span class="op">=</span> <span class="st">'Unnamed: 0'</span>)</span>
<span id="cb10-13"><a href="#cb10-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-14"><a href="#cb10-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-15"><a href="#cb10-15" aria-hidden="true" tabindex="-1"></a>categories1 <span class="op">=</span> {<span class="st">'cuisine'</span>: [<span class="st">'italian'</span>, <span class="st">'asian'</span>, <span class="st">'american'</span>]}</span>
<span id="cb10-16"><a href="#cb10-16" aria-hidden="true" tabindex="-1"></a>categories <span class="op">=</span> pd.DataFrame(categories1)</span>
<span id="cb10-17"><a href="#cb10-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-18"><a href="#cb10-18" aria-hidden="true" tabindex="-1"></a><span class="co"># Import process from thefuzz</span></span>
<span id="cb10-19"><a href="#cb10-19" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> thefuzz <span class="im">import</span> process</span>
<span id="cb10-20"><a href="#cb10-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-21"><a href="#cb10-21" aria-hidden="true" tabindex="-1"></a><span class="co"># Store the unique values of cuisine_type in unique_types</span></span>
<span id="cb10-22"><a href="#cb10-22" aria-hidden="true" tabindex="-1"></a>unique_types <span class="op">=</span> restaurants[<span class="st">'cuisine_type'</span>].unique()</span>
<span id="cb10-23"><a href="#cb10-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-24"><a href="#cb10-24" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate similarity of 'asian' to all values of unique_types</span></span>
<span id="cb10-25"><a href="#cb10-25" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(process.extract(<span class="st">'asian'</span>, unique_types, limit <span class="op">=</span> <span class="bu">len</span>(unique_types)))</span>
<span id="cb10-26"><a href="#cb10-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-27"><a href="#cb10-27" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate similarity of 'american' to all values of unique_types</span></span>
<span id="cb10-28"><a href="#cb10-28" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(process.extract(<span class="st">'american'</span>, unique_types, limit <span class="op">=</span> <span class="bu">len</span>(unique_types)))</span>
<span id="cb10-29"><a href="#cb10-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-30"><a href="#cb10-30" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate similarity of 'italian' to all values of unique_types</span></span>
<span id="cb10-31"><a href="#cb10-31" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(process.extract(<span class="st">'italian'</span>, unique_types, limit <span class="op">=</span> <span class="bu">len</span>(unique_types)))</span>
<span id="cb10-32"><a href="#cb10-32" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb10-33"><a href="#cb10-33" aria-hidden="true" tabindex="-1"></a><span class="co"># Iterate through categories</span></span>
<span id="cb10-34"><a href="#cb10-34" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> cuisine <span class="kw">in</span> categories[<span class="st">'cuisine'</span>]:  </span>
<span id="cb10-35"><a href="#cb10-35" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Create a list of matches, comparing cuisine with the cuisine_type column</span></span>
<span id="cb10-36"><a href="#cb10-36" aria-hidden="true" tabindex="-1"></a>  matches <span class="op">=</span> process.extract(cuisine, restaurants[<span class="st">'cuisine_type'</span>], limit<span class="op">=</span>restaurants.shape[<span class="dv">0</span>])</span>
<span id="cb10-37"><a href="#cb10-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-38"><a href="#cb10-38" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Iterate through the list of matches</span></span>
<span id="cb10-39"><a href="#cb10-39" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> match <span class="kw">in</span> matches:</span>
<span id="cb10-40"><a href="#cb10-40" aria-hidden="true" tabindex="-1"></a>     <span class="co"># Check whether the similarity score is greater than or equal to 80</span></span>
<span id="cb10-41"><a href="#cb10-41" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> match[<span class="dv">1</span>] <span class="op">&gt;=</span> <span class="dv">80</span>:</span>
<span id="cb10-42"><a href="#cb10-42" aria-hidden="true" tabindex="-1"></a>      <span class="co"># If it is, select all rows where the cuisine_type is spelled this way, and set them to the correct cuisine</span></span>
<span id="cb10-43"><a href="#cb10-43" aria-hidden="true" tabindex="-1"></a>      restaurants.loc[restaurants[<span class="st">'cuisine_type'</span>] <span class="op">==</span> match[<span class="dv">0</span>], <span class="st">'cuisine_type'</span>] <span class="op">=</span> cuisine</span>
<span id="cb10-44"><a href="#cb10-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-45"><a href="#cb10-45" aria-hidden="true" tabindex="-1"></a><span class="co"># Iterate through categories (THE SAME AS ABOVE)</span></span>
<span id="cb10-46"><a href="#cb10-46" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> cuisine <span class="kw">in</span> categories[<span class="st">'cuisine'</span>]:  </span>
<span id="cb10-47"><a href="#cb10-47" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Create a list of matches, comparing cuisine with the cuisine_type column</span></span>
<span id="cb10-48"><a href="#cb10-48" aria-hidden="true" tabindex="-1"></a>  matches <span class="op">=</span> process.extract(cuisine, restaurants_new[<span class="st">'cuisine_type'</span>], limit<span class="op">=</span><span class="bu">len</span>(restaurants_new.cuisine_type))</span>
<span id="cb10-49"><a href="#cb10-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-50"><a href="#cb10-50" aria-hidden="true" tabindex="-1"></a><span class="co"># Iterate through categories (THE SAME AS ABOVE)</span></span>
<span id="cb10-51"><a href="#cb10-51" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> cuisine <span class="kw">in</span> categories[<span class="st">'cuisine'</span>]:  </span>
<span id="cb10-52"><a href="#cb10-52" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Create a list of matches, comparing cuisine with the cuisine_type column</span></span>
<span id="cb10-53"><a href="#cb10-53" aria-hidden="true" tabindex="-1"></a>  matches <span class="op">=</span> process.extract(cuisine, restaurants_new[<span class="st">'cuisine_type'</span>], limit<span class="op">=</span><span class="bu">len</span>(restaurants_new.cuisine_type))</span>
<span id="cb10-54"><a href="#cb10-54" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-55"><a href="#cb10-55" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Iterate through the list of matches</span></span>
<span id="cb10-56"><a href="#cb10-56" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> match <span class="kw">in</span> matches:</span>
<span id="cb10-57"><a href="#cb10-57" aria-hidden="true" tabindex="-1"></a>     <span class="co"># Check whether the similarity score is greater than or equal to 80</span></span>
<span id="cb10-58"><a href="#cb10-58" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> match[<span class="dv">1</span>] <span class="op">&gt;=</span> <span class="dv">80</span>:</span>
<span id="cb10-59"><a href="#cb10-59" aria-hidden="true" tabindex="-1"></a>      <span class="co"># If it is, select all rows where the cuisine_type is spelled this way, and set them to the correct cuisine</span></span>
<span id="cb10-60"><a href="#cb10-60" aria-hidden="true" tabindex="-1"></a>      restaurants_new.loc[restaurants_new[<span class="st">'cuisine_type'</span>] <span class="op">==</span> match[<span class="dv">0</span>], <span class="st">'cuisine_type'</span>] <span class="op">=</span> cuisine</span>
<span id="cb10-61"><a href="#cb10-61" aria-hidden="true" tabindex="-1"></a>      </span>
<span id="cb10-62"><a href="#cb10-62" aria-hidden="true" tabindex="-1"></a><span class="co"># Inspect the final result</span></span>
<span id="cb10-63"><a href="#cb10-63" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(restaurants[<span class="st">'cuisine_type'</span>].unique())</span>
<span id="cb10-64"><a href="#cb10-64" aria-hidden="true" tabindex="-1"></a><span class="co"># Inspect the final result</span></span>
<span id="cb10-65"><a href="#cb10-65" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(restaurants_new[<span class="st">'cuisine_type'</span>].unique())</span>
<span id="cb10-66"><a href="#cb10-66" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-67"><a href="#cb10-67" aria-hidden="true" tabindex="-1"></a><span class="co"># Create an indexer and object and find possible pairs</span></span>
<span id="cb10-68"><a href="#cb10-68" aria-hidden="true" tabindex="-1"></a>indexer <span class="op">=</span> recordlinkage.Index()</span>
<span id="cb10-69"><a href="#cb10-69" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-70"><a href="#cb10-70" aria-hidden="true" tabindex="-1"></a><span class="co"># Block pairing on cuisine_type</span></span>
<span id="cb10-71"><a href="#cb10-71" aria-hidden="true" tabindex="-1"></a>indexer.block(<span class="st">'cuisine_type'</span>)</span>
<span id="cb10-72"><a href="#cb10-72" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-73"><a href="#cb10-73" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate pairs</span></span>
<span id="cb10-74"><a href="#cb10-74" aria-hidden="true" tabindex="-1"></a>pairs <span class="op">=</span> indexer.index(restaurants, restaurants_new)</span>
<span id="cb10-75"><a href="#cb10-75" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-76"><a href="#cb10-76" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a comparison object</span></span>
<span id="cb10-77"><a href="#cb10-77" aria-hidden="true" tabindex="-1"></a>comp_cl <span class="op">=</span> recordlinkage.Compare()</span>
<span id="cb10-78"><a href="#cb10-78" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-79"><a href="#cb10-79" aria-hidden="true" tabindex="-1"></a><span class="co"># Find exact matches on city, cuisine_types </span></span>
<span id="cb10-80"><a href="#cb10-80" aria-hidden="true" tabindex="-1"></a>comp_cl.exact(<span class="st">'city'</span>, <span class="st">'city'</span>, label<span class="op">=</span><span class="st">'city'</span>)</span>
<span id="cb10-81"><a href="#cb10-81" aria-hidden="true" tabindex="-1"></a>comp_cl.exact(<span class="st">'cuisine_type'</span>, <span class="st">'cuisine_type'</span>, label <span class="op">=</span> <span class="st">'cuisine_type'</span>)</span>
<span id="cb10-82"><a href="#cb10-82" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-83"><a href="#cb10-83" aria-hidden="true" tabindex="-1"></a><span class="co"># Find similar matches of rest_name</span></span>
<span id="cb10-84"><a href="#cb10-84" aria-hidden="true" tabindex="-1"></a>comp_cl.string(<span class="st">'name'</span>, <span class="st">'name'</span>, label<span class="op">=</span><span class="st">'name'</span>, threshold <span class="op">=</span> <span class="fl">0.8</span>) </span>
<span id="cb10-85"><a href="#cb10-85" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-86"><a href="#cb10-86" aria-hidden="true" tabindex="-1"></a><span class="co"># Get potential matches and print</span></span>
<span id="cb10-87"><a href="#cb10-87" aria-hidden="true" tabindex="-1"></a>potential_matches <span class="op">=</span> comp_cl.compute(pairs, restaurants, restaurants_new)</span>
<span id="cb10-88"><a href="#cb10-88" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(potential_matches)</span>
<span id="cb10-89"><a href="#cb10-89" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-90"><a href="#cb10-90" aria-hidden="true" tabindex="-1"></a><span class="co"># Isolate potential matches with row sum &gt;=3</span></span>
<span id="cb10-91"><a href="#cb10-91" aria-hidden="true" tabindex="-1"></a>matches <span class="op">=</span> potential_matches[potential_matches.<span class="bu">sum</span>(axis <span class="op">=</span> <span class="dv">1</span>) <span class="op">&gt;=</span> <span class="dv">3</span>]</span>
<span id="cb10-92"><a href="#cb10-92" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-93"><a href="#cb10-93" aria-hidden="true" tabindex="-1"></a><span class="co"># For this example, tightening your selection criteria will ensure good duplicate finds!</span></span>
<span id="cb10-94"><a href="#cb10-94" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-95"><a href="#cb10-95" aria-hidden="true" tabindex="-1"></a><span class="co"># Get values of second column index of matches</span></span>
<span id="cb10-96"><a href="#cb10-96" aria-hidden="true" tabindex="-1"></a>matching_indices <span class="op">=</span> matches.index.get_level_values(<span class="dv">1</span>)</span>
<span id="cb10-97"><a href="#cb10-97" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-98"><a href="#cb10-98" aria-hidden="true" tabindex="-1"></a><span class="co"># Subset restaurants_new based on non-duplicate values</span></span>
<span id="cb10-99"><a href="#cb10-99" aria-hidden="true" tabindex="-1"></a>non_dup <span class="op">=</span> restaurants_new[<span class="op">~</span>restaurants_new.index.isin(matching_indices)]</span>
<span id="cb10-100"><a href="#cb10-100" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-101"><a href="#cb10-101" aria-hidden="true" tabindex="-1"></a><span class="co"># Append non_dup to restaurants</span></span>
<span id="cb10-102"><a href="#cb10-102" aria-hidden="true" tabindex="-1"></a>full_restaurants <span class="op">=</span> pd.concat([restaurants, non_dup], ignore_index<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb10-103"><a href="#cb10-103" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(full_restaurants)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>[('asian', 100), ('asiane', 91), ('asiann', 91), ('asiian', 91), ('asiaan', 91), ('asianne', 83), ('asiat', 80), ('italiann', 72), ('italiano', 72), ('italianne', 72), ('italiaan', 68), ('italiian', 68), ('itallian', 68), ('italian', 67), ('amurican', 62), ('american', 62), ('ameerican', 60), ('aamerican', 60), ('ameriican', 60), ('amerrican', 60), ('ameerrican', 60), ('ammereican', 60), ('americann', 57), ('americano', 57), ('ammericann', 54), ('americin', 51), ('amerycan', 51), ('america', 50), ('merican', 50)]
[('american', 100), ('ameerican', 94), ('aamerican', 94), ('ameriican', 94), ('amerrican', 94), ('americann', 94), ('americano', 94), ('america', 93), ('merican', 93), ('ameerrican', 89), ('ammereican', 89), ('ammericann', 89), ('amurican', 88), ('americin', 88), ('amerycan', 88), ('asian', 62), ('asiane', 57), ('asiann', 57), ('asiian', 57), ('asiaan', 57), ('asianne', 53), ('italian', 53), ('italiann', 50), ('italiano', 50), ('italiaan', 50), ('italiian', 50), ('itallian', 50), ('italianne', 47), ('asiat', 46)]
[('italian', 100), ('italiann', 93), ('italiano', 93), ('italiaan', 93), ('italiian', 93), ('itallian', 93), ('italianne', 88), ('asian', 67), ('asiane', 62), ('asiann', 62), ('asiian', 62), ('asiaan', 62), ('asianne', 57), ('amurican', 53), ('american', 53), ('ameerican', 50), ('aamerican', 50), ('ameriican', 50), ('amerrican', 50), ('americann', 50), ('americano', 50), ('asiat', 50), ('ameerrican', 47), ('ammereican', 47), ('ammericann', 47), ('america', 43), ('merican', 43), ('americin', 40), ('amerycan', 40)]
['american' 'asian' 'italian']
['american' 'asian' 'italian']
        city  cuisine_type  name
0   0      0             1   0.0
    1      0             1   0.0
    2      0             1   0.0
    3      1             1   0.0
    4      0             1   0.0
...      ...           ...   ...
335 75     0             1   0.0
    76     0             1   0.0
    77     0             1   0.0
    78     0             1   0.0
    79     0             1   0.0

[10991 rows x 3 columns]
                          name                       addr               city  \
0    arnie morton's of chicago   435 s. la cienega blv .         los angeles   
1           art's delicatessen       12224 ventura blvd.         studio city   
2                    campanile       624 s. la brea ave.         los angeles   
3                        fenix    8358 sunset blvd. west           hollywood   
4           grill on the alley           9560 dayton way         los angeles   
..                         ...                        ...                ...   
405                      feast        1949 westwood blvd.            west la   
406                   mulberry        17040 ventura blvd.             encino   
407                 matsuhissa   129 n. la cienega blvd.       beverly hills   
408                    jiraffe      502 santa monica blvd       santa monica   
409                   martha's  22nd street grill 25 22nd  st. hermosa beach   

          phone         type cuisine_type  
0    3102461501     american     american  
1    8187621221     american     american  
2    2139381447     american     american  
3    2138486677     american     american  
4    3102760615     american     american  
..          ...          ...          ...  
405  3104750400      chinese        asian  
406  8189068881        pizza        asian  
407  3106599639        asian        asian  
408  3109176671  californian      italian  
409  3103767786     american      italian  

[410 rows x 6 columns]</code></pre>
</div>
</div>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-1-contents" aria-controls="callout-1" aria-expanded="true" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Important
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-1" class="callout-1-contents callout-collapse collapse show">
<div class="callout-body-container callout-body">
<p>Another code that is similar to the one in the previous code</p>
<div class="sourceCode" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Iterate through categories (THE SAME AS ABOVE)</span></span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> cuisine <span class="kw">in</span> categories:  </span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Create a list of matches, comparing cuisine with the cuisine_type column</span></span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a>  matches <span class="op">=</span> process.extract(cuisine, restaurants[<span class="st">'cuisine_type'</span>], limit<span class="op">=</span><span class="bu">len</span>(restaurants.cuisine_type))</span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Iterate through the list of matches</span></span>
<span id="cb12-7"><a href="#cb12-7" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> match <span class="kw">in</span> matches:</span>
<span id="cb12-8"><a href="#cb12-8" aria-hidden="true" tabindex="-1"></a>     <span class="co"># Check whether the similarity score is greater than or equal to 80</span></span>
<span id="cb12-9"><a href="#cb12-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> match[<span class="dv">1</span>] <span class="op">&gt;=</span> <span class="dv">80</span>:</span>
<span id="cb12-10"><a href="#cb12-10" aria-hidden="true" tabindex="-1"></a>      <span class="co"># If it is, select all rows where the cuisine_type is spelled this way, and set them to the correct cuisine</span></span>
<span id="cb12-11"><a href="#cb12-11" aria-hidden="true" tabindex="-1"></a>      restaurants.loc[restaurants[<span class="st">'cuisine_type'</span>] <span class="op">==</span> match[<span class="dv">0</span>]] <span class="op">=</span> cuisine</span>
<span id="cb12-12"><a href="#cb12-12" aria-hidden="true" tabindex="-1"></a>      </span>
<span id="cb12-13"><a href="#cb12-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Inspect the final result</span></span>
<span id="cb12-14"><a href="#cb12-14" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(restaurants[<span class="st">'cuisine_type'</span>].unique())</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</div>
</div>
</section>
</section>
<section id="explore-datasets" class="level2">
<h2 class="anchored" data-anchor-id="explore-datasets">EXPLORE DATASETS</h2>
<p>Use the DataFrames imported in the first cell to explore the data and practice your skills!</p>
<ul>
<li><p>For each DataFrame, inspect the data types of each column and, where needed, clean and convert columns into the correct data type. You should also rename any columns to have more descriptive titles.</p></li>
<li><p>Identify and remove all the duplicate rows in <code>ride_sharing</code>.</p></li>
<li><p>Inspect the unique values of all the columns in <code>airlines</code> and clean any inconsistencies.</p></li>
<li><p>For the <code>airlines</code> DataFrame, create a new column called <code>International</code> from <code>dest_region</code>, where values representing US regions map to <code>False</code> and all other regions map to <code>True</code>.</p></li>
<li><p>The <code>banking</code> DataFrame contains out of date ages. Update the <code>Age</code> column using today’s date and the <code>birth_date</code> column.</p></li>
<li><p>Clean the <code>restaurants_new</code> DataFrame so that it better matches the categories in the <code>city</code> and <code>type</code> column of the <code>restaurants</code> DataFrame. Afterward, given typos in restaurant names, use record linkage to generate possible pairs of rows between <code>restaurants</code> and <code>restaurants_new</code> using criteria you think is best.</p></li>
</ul>
</section>
</section>

</main>
<!-- /main column -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>